# ch 26

The literature of the fantastic abounds in inanimate objects magically endowed with sentience and the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s Frankenstein, there is something deeply moving about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so life like that he tapped it on the knee and commanded it to speak. Perhaps this shouldn’t be surprising. Language is the mark conversation of humanity and sentience, and conversation or dialogue dialogue is the most fundamental and specially privileged arena of language. It is the first kind of language we learn as children, and for most of us, it is the kind of language we most commonly indulge in, whether we are ordering curry for lunch or buying spinach, participating in business meetings or talking with our families, booking airline flights or complaining about the weather.

무생물에 대한 환상적인 문헌은 마술과 선의의 선물로 마술을 부여했습니다. 오비드의 피그말리온 동상에서 메리 셸리의 프랑켄슈타인에 이르기까지 무언가를 만들고 대화를 나누는 데 깊은 감동이 있습니다. 전설에 따르면 미켈란젤로는 조각 모세를 마친 후에 무릎을 꿇고 말하기를 마치는 것처럼 인생을 그렇게 생각했습니다. 아마도 이것은 놀라운 일이 아닙니다. 언어는 인류와 관용의 마크 대화이며, 대화 또는 대화 대화는 가장 기본적이고 특권이있는 언어 영역입니다. 점심은 카레를 주문하든 시금치를 사 든가, 비즈니스 미팅에 참여하거나 가족과 대화를 나눌 때 가장 많이 사용하는 언어입니다. , 항공사 항공편 예약 또는 날씨에 대한 불만.

This chapter introduces the fundamental algorithms of conversational agents, conversational agent dialogue system or dialogue systems. These programs communicate with users in natural language(text, speech, or both), and fall into two classes. Task-oriented dialogue agents use conversation with users to help complete tasks. Dialogue agents in digital assistants(Siri, Alexa, Google Now/Home, Cortana, etc.), give directions, control appliances, find restaurants, or make calls. Conversational agents can answer questions on corporate websites, interface with robots, and even be used for social good: DoNotPay is a “robot lawyer” that helps people challenge incorrect parking fines, apply for emergency housing, or claim asylum if they are refugees. By contrast, chatbots are systems designed for extended conversations, set up to mimic the unstructured conversations or ‘chats’ characteristic of human-human interaction, mainly for entertainment, but also for practical purposes like making task-oriented agents more natural. In Section 26.2 we’ll discuss the three major chatbot architectures : rulebased systems, information retrieval systems, and encoder-decoder models. In Section 26.3 we turn to task-oriented agents, introducing the frame-based architecture (the GUS architecture) that underlies most modern task-based systems.

이 장에서는 대화 에이전트, 대화 에이전트 대화 시스템 또는 대화 시스템의 기본 알고리즘을 소개합니다. 이 프로그램은 자연 언어 (텍스트, 음성 또는 둘 다)로 사용자와 통신하며 두 가지 클래스로 나뉩니다. 작업 지향 대화 에이전트는 사용자와의 대화를 사용하여 작업을 완료합니다. 디지털 비서 (Siri, Alexa, Google Now / Home, Cortana 등)의 대화 상담원은 길 찾기, 기기 제어, 식당 찾기 또는 전화 걸기 등을 수행합니다. 대화 상담원은 회사 웹 사이트에 대한 질문에 답변하고 로봇과의 인터페이스를 제공하며 사회적 이익을 위해 사용될 수도 있습니다. DoNotPay는 사람들이 잘못된 주차 벌금에 이의를 제기하거나 긴급 주택을 신청하거나 난민 인 경우 망명을 신청하는 데 도움이되는 "로봇 변호사"입니다. 반대로 챗봇은 확장 된 대화를 위해 고안된 시스템으로, 주로 엔터테인먼트를 위해뿐만 아니라 업무 지향적 인 상담원을보다 자연스럽게 만드는 것과 같은 실제적인 목적을 위해 구조화되지 않은 대화 또는 인간과 인간의 상호 작용의 '채팅'을 모방하도록 설정되었습니다. 26.2 절에서는 규칙 기반 시스템, 정보 검색 시스템 및 인코더 디코더 모델의 세 가지 주요 챗봇 아키텍처에 대해 설명합니다. 26.3 절에서 우리는 작업 지향 에이전트로 돌아가서 가장 현대적인 작업 기반 시스템의 기본이되는 프레임 기반 아키텍처 (GUS 아키텍처)를 소개합니다.

##  Properties of Human Conversation

Conversation between humans is an intricate and complex joint activity. Before we attempt to design a conversational agent to converse with humans, it is crucial to understand something about how humans converse with each other. Consider some of the phenomena that occur in the conversation between a human travel agent and a human client excerpted in Fig. 26.1.

인간 간의 대화는 복잡하고 복잡한 공동 활동입니다. 인간과 대화 할 대화 에이전트를 디자인하기 전에 인간이 서로 대화하는 방법에 대해 이해하는 것이 중요합니다. 그림 26.1에서 발췌 한 인간 여행사와 인간 고객 간의 대화에서 발생하는 일부 현상을 고려하십시오.

```
C1: . . . I need to travel in May.
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
A2: And you’re flying into what city?
C3: Seattle.
A3: And what time would you like to leave Pittsburgh?
C4: Uh hmm I don’t think there’s many options for non-stop.
A4: Right. There’s three non-stops today.
C5: What are they?
A5: The first one departs PGH at 10:00am arrives Seattle at 12:05 their time. The second flight departs PGH at 5:55pm, arrives Seattle at 8pm. And the last
flight departs PGH at 8:15pm arrives Seattle at 10:28pm.
C6: OK I’ll take the 5ish flight on the night before on the 11th.
A6: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air flight 115.
C7: OK.
A7: And you said returning on May 15th?
C8: Uh, yeah, at the end of the day.
A8: OK. There’s #two non-stops . . . #
C9: #Act. . . actually #, what day of the week is the 15th?
A9: It’s a Friday.
C10: Uh hmm. I would consider staying there an extra day til Sunday.
A10: OK. . . OK. On Sunday I have . . .
```

### Turns

A dialogue is a sequence of turns (A1, B1, A2, and so on), each a single contribution to the dialogue (as if in a game: I take a turn, then you take a turn, then me, and so on). A turn can consist of a sentence (like C1), although it might be as short as a single word (C7) or as long as multiple sentences (A5).

대화는 일련의 회전 (A1, B1, A2 등)이며, 각각 대화에 단일 기여를합니다 (게임에서와 같이 : 차례를 한 다음 차례를 한 다음 나 등) ). 턴은 한 단어 (C7)만큼 짧거나 여러 문장 (A5)만큼 길지만 문장 (예 : C1)으로 구성 될 수 있습니다.

Turn structure has important implications for spoken dialogue. A system has to know when to stop talking; the client interrupts (in A8 and C9), so the system must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are able to (most of the time) detect when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This task endpointing called endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns.

턴 구조는 대화에 중요한 영향을 미칩니다. 시스템은 언제 말을 멈추어야하는지 알아야합니다. 클라이언트가 (A8 및 C9에서) 인터럽트하므로 시스템은 대화를 중지해야하며 사용자가 정정 할 수 있음을 알아야합니다. 시스템은 언제 이야기를 시작할 지 알아야합니다. 예를 들어, 대부분의 대화에서, 스피커는 상대방이 대화를 마치려고 할 때 사람들이 (대부분) 감지 할 수 있기 때문에 다른 스피커가 끝난 후 거의 즉시 자신의 차례를 시작합니다. 음성 대화 시스템은 사용자의 발화 여부를 감지하여 발화를 처리하고 응답 할 수 있어야합니다. 엔드 포인트 또는 엔드 포인트 감지라고하는이 작업 엔드 포인트는 소음 때문에 사람들이 종종 턴 중간에 일시 정지하기 때문에 상당히 어려울 수 있습니다.

A key insight into conversation—due originally to the philosopher Wittgenstein (1953) but worked out more fully by Austin (1962)—is that each utterance in a dialogue is a kind of action being performed by the speaker. These actions are comspeech acts monly called speech acts or dialog acts: here’s one taxonomy consisting of 4 major classes (Bach and Harnish, 1979):

원래 철학자 Wittgenstein (1953)에 의해 대화되었지만 오스틴 (1962)에 의해 더욱 완벽하게 해결 된 대화에 대한 주요 통찰은 대화의 각 발화가 화자에 의해 수행되는 일종의 행동이라는 것입니다. 이러한 행동은 언어 행위 또는 대화 행위라고하는 징계 행위입니다. 다음은 4 가지 주요 클래스로 구성된 분류 체계입니다 (Bach and Harnish, 1979).

```
Constatives: committing the speaker to something’s being the case (answering, claiming,
confirming, denying, disagreeing, stating)
Directives: attempts by the speaker to get the addressee to do something (advising, asking, forbidding, inviting, ordering, requesting)
Commissives: committing the speaker to some future course of action (promising, planning,
vowing, betting, opposing)
Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some social action (apologizing, greeting, thanking, accepting an acknowledgment)
```

```
답변 : 화자에게 사건에 대해 답변하기 (답변, 주장, 확인, 거부, 동의하지 않음, 진술)
지시어 : 화자에 의해 수취인이 무언가를하도록 시도 (조언, 요청, 금지, 초대, 주문, 요청)
커미티 브 : 발표자에게 미래의 행동 과정 (약속, 계획, 투표, 베팅, 반대)
감사의 말 : 사회적 행동 (사죄, 인사, 감사, 인정)과 관련하여 청중에 대한 화자의 태도를 표현합니다.
```

A user asking a person or a dialogue system to do something (‘Turn up the music’) is issuing a DIRECTIVE. Asking a question that requires an answer is also a way of issuing a DIRECTIVE: in a sense when the system says (C2) “what day in May did you want to travel?” it’s as if the system is (very politely) commanding the system to answer. By contrast, a user stating a constraint (like C1 ‘I need to travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing an ACKNOWLEDGMENT. The speech act expresses an important component of the intention of the speaker (or writer) in saying what they said.

개인이나 대화 시스템에 무언가를 요청하는 ( '음악 켜기') 사용자가 DIRECTIVE를 발행하고 있습니다. 답변이 필요한 질문을하는 것도 DIRECTIVE를 발행하는 방법입니다. 시스템이 (C2)“5 월 어느 날 여행하고 싶습니까?” 마치 시스템이 (매우 정중하게) 시스템에 응답하도록 명령하는 것처럼 보입니다. 대조적으로 C1‘5 월에 여행해야 해요’와 같은 제약 조건을 명시한 사용자는 CONSTATIVE를 발행하고 있습니다. 시스템에 감사하는 사용자가 ACKNOWLEDGMENT를 발행하고 있습니다. 발화 행위는 발언자 (또는 작가)가 자신이 말한 것을 말하려는 의도의 중요한 구성 요소를 나타냅니다.


### Grounding

A dialogue is not just a series of independent speech acts, but rather a collective act performed by the speaker and the hearer. Like all collective acts, it’s important for the participants to establish what they both agree on, called the common ground common ground grounding (Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Grounding means acknowledging that the hearer has understood the speaker; like an ACK used to confirm receipt in data communications (Clark, 1996). (People need grounding for non-linguistic actions as well; the reason an elevator button lights up when it’s pressed is to acknowledge that the elevator has indeed been called (Norman, 1988).)

대화는 일련의 독립적 인 연설 행위가 아니라 스피커와 청취자가 수행하는 집단적 행위입니다. 모든 집단 행위와 마찬가지로 참가자는 공통 접지 공통 접지라고하는 두 가지 모두에 동의하는 것을 설정하는 것이 중요합니다 (Stalnaker, 1978). 화자는 서로의 발화를 접지하여이를 수행합니다. 접지는 청취자가 스피커를 이해했음을 인정하는 것입니다. 데이터 통신 수신을 확인하는 데 사용되는 ACK처럼 (Clark, 1996). (사람들은 비언어적 행동을 위해서도 접지가 필요하다; 엘리베이터 버튼을 눌렀을 때 엘리베이터 버튼이 점등되는 이유는 엘리베이터가 실제로 호출되었다는 것을 인정하기 위해서이다 (Norman, 1988).)

Humans constantly ground each other’s utterances. We can ground by explicitly saying “OK”, as the agent does in A8 or A10. Or we can ground by repeating what the other person says; in utterance A1 the agent repeats “in May”, demonstrating her understanding to the client. Or notice that when the client answers a question, the agent begins the next question with “And”. The “And” implies that the new question is ‘in addition’ to the old question, again indicating to the client that the agent has successfully understood the answer to the last question.

인간은 끊임없이 서로의 발화를 연마합니다. 상담원이 A8 또는 A10에서하는 것처럼“OK”라고 명시 적으로 말하면됩니다. 아니면 상대방이 말한 것을 반복함으로써 근거를 세울 수 있습니다. 발언 A1에서 상담원은 "5 월에"반복하여 고객에 대한 이해를 보여줍니다. 또는 고객이 질문에 대답하면 상담원은“And”로 다음 질문을 시작합니다. "그리고"는 새로운 질문이 이전 질문에 '추가 된 것'임을 암시하며 상담원이 마지막 질문에 대한 답변을 성공적으로 이해했음을 다시 고객에게 나타냅니다.


### Subdialogues and Dialogue Structure

Conversations have structure. Consider, for example, the local structure betweenspeech acts discussed in the field of conversational analysis (Sacks et al., 1974).conversationalanalysisQUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed byACCEPTANCE (or REJECTION). COMPLIMENTS (“Nice jacket!”) often give rise toadjacency pair DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs arecomposed of a first pair part and a second pair part (Schegloff, 1968), and theseexpectations can help systems decide what actions to take.

대화 구조가 있습니다. 예를 들어, 대화 분석 분야에서 논의 된 음성 행위 사이의 지역적 구조를 고려해보십시오 (Sacks et al., 1974). 제안서에는 수락 (또는 거부)이 따른다. 칭찬 (“좋은 재킷!”)은 종종 인접 쌍 다운 플레이어 (“아,이 오래된 것?”)를 일으 킵니다. 인접 쌍이라고하는 이들 쌍은 제 1 쌍 부분과 제 2 쌍 부분으로 구성되며 (Schegloff, 1968), 이러한 기대는 시스템이 수행 할 조치를 결정하는 데 도움이 될 수 있습니다.

However, dialogue acts aren’t always followed immediately by their second pairside sequence part. The two parts can be separated by a side sequence (Jefferson 1972) or subsubdialogue dialogue. For example utterances C9 to A10 constitute a correction subdialogue(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):

그러나 대화 행위가 항상 두 번째 쌍측 순서 부분 바로 뒤에 오는 것은 아닙니다. 두 부분은 사이드 시퀀스 (Jefferson 1972) 또는 서브 서브 다이얼로그 대화에 의해 분리 될 수 있습니다. 예를 들어, 발화 C9에서 A10은 수정 서브 다이얼로그 (Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998)를 구성합니다.

```
C9: #Act. . . actually#, what day of the week is the 15th?
A9: It’s a Friday.
C10: Uh hmm. I would consider staying there an extra day til Sunday.
A10: OK. . . OK. On Sunday I have . . .
```

The question in C9 interrupts the prior discourse, in which the agent was lookingfor a May 15 return flight. The agent must answer the question and also realize that‘’I would consider staying...til Sunday” means that the client would probably like tochange their plan, and now go back to finding return flights, but for the 17th.Another side sequence is the clarification question, which can form a subdialogue between a REQUEST and a RESPONSE. This is especially common in dialogue systems where speech recognition errors causes the system to have to ask for clarifications or repetitions like the following:

C9의 질문은 요원이 5 월 15 일 귀국 비행기를 찾고 있던 이전 담화를 중단시킵니다. 상담원은 질문에 대답하고 '일요일까지 머무를 것을 고려합니다'라는 말은 고객이 계획을 변경하고 다시 귀국 항공편을 찾는 중이지만 17 일에는 다시 돌아온다는 것을 의미합니다. 설명 질문이며, 요청과 응답 사이에 하위 대화 상자를 형성 할 수 있습니다. 이것은 음성 인식 오류로 인해 시스템이 다음과 같이 설명이나 반복을 요구해야하는 대화 시스템에서 특히 일반적입니다.

```
User: What do you have going to UNKNOWN WORD on the 5th?
System: Let’s see, going where on the 5th?
User: Going to Hong Kong.
System: OK, here are some flights...
```

In addition to side-sequences, questions often have presequences, like the following example where a user starts with a question about the system’s capabilities(“Can you make train reservations”) before making a request.

부가적인 순서 외에도, 질문은 종종 사용자가 요청하기 전에 시스템 기능 (“기차 예약 가능”)에 대한 질문으로 시작하는 다음 예와 같은 사전 순서를 갖습니다.

```
User: Can you make train reservations?
System: Yes I can.
User: Great, I’d like to reserve a seat on the 4pm train to New York
```

### Initiative

Sometimes a conversation is completely controlled by one participant. For examplea reporter interviewing a chef might ask questions, and the chef responds. We say initiative that the reporter in this case has the conversational initiative (Walker and Whittaker,1990). In normal human-human dialogue, however, it’s more common for initiativeto shift back and forth between the participants, as they sometimes answer questions,sometimes ask them, sometimes take the conversations in new directions, sometimes not. You may ask me a question, and then I respond asking you to clarify somethingyou said, which leads the conversation in all sorts of ways. We call such interactions mixed initiative.

때때로 대화는 한 참가자가 완전히 제어합니다. 예를 들어 요리사를 인터뷰하는 기자가 질문을하고 요리사가 응답합니다. 우리는이 경우에 기자가 대화 이니셔티브를 가지고 있다는 이니셔티브를 말한다 (Walker and Whittaker, 1990). 그러나 일반적인 인간과 인간의 대화에서는 참가자들이 때때로 질문에 대답하고 때로는 질문하거나 때로는 새로운 방향으로 대화를 나누는 경우가 종종 있습니다. 당신은 저에게 질문을 할 수 있습니다. 그리고 나는 당신이 말한 것을 분명히하도록 요청하며, 그것은 모든 종류의 방식으로 대화를 이끌어냅니다. 우리는 그러한 상호 작용을 혼합 이니셔티브라고 부릅니다.

Mixed initiative, while the norm for human-human conversations, is very difficult for dialogue systems to achieve. It’s much easier to design dialogue systems tobe passive responders. In the question answering systems we saw in Chapter 25, or in simple search engines, the initiative lies completely with the user. In such user initiative systems, the user specifies a query, and the systems responds. Then theuser can specify another query. Alternatively, you may have had the experience of being stuck in a bad dialogue system that asks a question and gives you no opportunity to do anything until you answer it. Such system-initiative architectures can bevery frustrating

인간과 인간의 대화에 대한 표준 인 혼합 이니셔티브는 대화 시스템이 달성하기가 매우 어렵습니다. 대화 시스템을 수동 응답자가되도록 훨씬 쉽게 설계 할 수 있습니다. 25 장이나 간단한 검색 엔진에서 보았던 질문 응답 시스템에서 이니셔티브는 사용자에게 전적으로 달려 있습니다. 이러한 사용자 이니셔티브 시스템에서 사용자는 쿼리를 지정하고 시스템이 응답합니다. 그런 다음 사용자는 다른 쿼리를 지정할 수 있습니다. 다른 방법으로, 당신은 나쁜 대화 시스템에 갇힌 경험이 있었을 수도 있습니다. 이러한 시스템 시작 아키텍처는 매우 실망 스러울 수 있습니다.


### Inference and Implicature

Inference is also important in dialogue understanding. Consider the client’s response C2, repeated here:

```
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
```

Notice that the client does not in fact answer the agent’s question. The client merely mentions a meeting at a certain time. What is it that licenses the agent to infer that the client is mentioning this meeting so as to inform the agent of the travel dates?

고객은 실제로 상담원의 질문에 답변하지 않습니다. 클라이언트는 단지 특정 시간에 회의를 언급합니다. 상담원에게 여행 날짜를 알리기 위해 고객이이 미팅을 언급하고 있다고 유추 할 수있는 것은 무엇입니까?

The speaker seems to expect the hearer to draw certain inferences; in other words, the speaker is communicating more information than seems to be present in the uttered words. This kind of example was pointed out by Grice (1975, 1978) implicature as part of his theory of conversational implicature. Implicature means a particular class of licensed inferences. Grice proposed that what enables hearers to draw these inferences is that conversation is guided by a set of maxims, general heuristics that play a guiding role in the interpretation of conversational utterances. One such maxim is the maxim of relevance which says that speakers attempt to be relevant,they don’t just utter random speech acts. When the client mentions a meeting on the12th, the agent reasons ‘There must be some relevance for mentioning this meeting. What could it be?’. The agent knows that one precondition for having a meeting(at least before Web conferencing) is being at the place where the meeting is held, and therefore that maybe the meeting is a reason for the travel, and if so, then since people like to arrive the day before a meeting, the agent should infer that the flight should be on the 11th.

화자는 청중이 특정한 추론을 이끌어 내기를 기대하는 것 같습니다. 다시 말해, 화자는 말로 표현 된 것보다 더 많은 정보를 전달하고 있습니다. 이런 종류의 예는 그의 대화 적 함의 이론의 일부로서 Grice (1975, 1978)의 함축에 의해 지적되었다. 의미는 라이센스 된 추론의 특정 클래스를 의미합니다. 그리스 (Grice)는 청중들이 이러한 추론을 이끌어 낼 수있게하는 것은 대화가 통역을 해석하는 데있어 중요한 역할을하는 최대의 일반적인 휴리스틱에 의해 대화가 유도된다고 제안했다. 그러한 최대치 중 하나는 화자가 적절하다고 말하면서 무작위로 말하는 말만하는 것이 아니라고 말하는 관련성의 최대치입니다. 고객이 12 일에 회의에 대해 언급 할 때 상담원은``이 회의에 대해 언급해야합니다. 뭐가 될수 있었는지?'. 상담원은 회의를 개최하기위한 장소 (최소한 웹 회의 전)에 대한 하나의 전제 조건이 진행 중이므로 회의가 여행의 이유 일 수 있다는 것을 알고 있습니다. 회의 전날에 도착하는 경우, 에이전트는 비행이 11 일에 있어야한다고 추론해야합니다.

These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research.

인간 대화의 이러한 미묘한 특성 (턴, 스피치 행위, 접지, 대화 구조, 주도 및 함축)은 인간과 자연스럽게 대화 할 수있는 대화 시스템을 구축하기 어려운 이유 중 하나입니다. 이러한 과제 중 다수는 대화 시스템 연구의 활발한 분야입니다.

## Chatbots

The simplest kinds of dialogue systems are chatbots, systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or ‘chats’ characteristic of informal human-human interaction.

가장 간단한 종류의 대화 시스템은 구조화되지 않은 대화를 모방하거나 비공식적 인 인간-인간 상호 작용의 '채팅'특성을 목표로 확장 된 대화를 수행 할 수있는 챗봇입니다.

These systems often have an entertainment value, such as Cleverbot, a simple IR based chatbot capable of carrying on the kinds of conversations shown in Fig. 26.2,or Microsoft’s XiaoIce system1(Zhou et al., 2018), a more sophisticated commercial system that chats with people on text messaging platforms (Fig. 26.3). Yet starting from the very first system, ELIZA (Weizenbaum, 1966), chatbots have also been used for practical purposes, such as testing theories of psychological counseling.

이러한 시스템은 그림 26.2에 표시된 종류의 대화를 수행 할 수있는 간단한 IR 기반 챗봇 인 Cleverbot과 같은 엔터테인먼트 가치 또는 종종보다 정교한 상용 시스템 인 Microsoft의 XiaoIce 시스템 1 (Zhou et al., 2018)과 같은 엔터테인먼트 가치를 갖습니다. 문자 메시지 플랫폼 사용자와 채팅합니다 (그림 26.3). 그러나 최초의 시스템 인 ELIZA (Weizenbaum, 1966)부터 챗봇은 심리 상담 이론 테스트와 같은 실용적인 목적으로도 사용되었습니다.

Like practically everything else in language processing, chatbot architectures fall into two classes: rule-based systems and corpus-based systems. Rule-based systems include the early influential ELIZA and PARRY systems. Corpus-based systems mine large datasets of human-human conversations, which can be done by using information retrieval (IR-based systems simply copy a human’s response from a previous conversation) or by using a machine translation paradigm such as neural network sequence-to-sequence systems, to learn to map from a user utterance to a system response.

언어 처리의 다른 모든 것과 마찬가지로 챗봇 아키텍처는 규칙 기반 시스템과 코퍼스 기반 시스템의 두 가지 클래스로 분류됩니다. 규칙 기반 시스템에는 초기 영향력있는 ELIZA 및 PARRY 시스템이 포함됩니다. 코퍼스 기반 시스템은 정보 검색 (IR 기반 시스템은 단순히 이전 대화에서 인간의 응답을 복사 함)을 사용하거나 신경망 시퀀스와 같은 기계 번역 패러다임을 사용하여 수행 할 수있는 인간-인간 대화의 대규모 데이터 세트를 채굴합니다. -시퀀스 시스템-사용자 발화에서 시스템 응답으로 매핑하는 방법을 배웁니다.

### 26.2.1 Rule-based chatbots: ELIZA and PARRY

ELIZA (Weizenbaum, 1966) is the most important chatbot dialogue system in thehistory of the field, and so we take some time here to describe its algorithm in detail. ELIZA was designed to simulate a Rogerian psychologist, based on a branchof clinical psychology whose methods involve drawing the patient out by reflectingpatient’s statements back at them. Rogerian psychology is the rare type of conversation in which, as Weizenbaum points out, one can “assume the pose of knowingalmost nothing of the real world”. If a patient says “I went for a long boat ride” andthe psychiatrist says “Tell me about boats”, you don’t assume she didn’t know whata boat is, but rather assume she had some conversational goal. Most chatbots tryingto pass the Turing test choose a domain with similar properties.

```
Men are all alike.
IN WHAT WAY
They’re always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
..
..
WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?
My father
YOUR FATHER
You are like my father in some ways.
WHAT RESEMBLANCE DO YOU SEE
You are not very aggressive but I think you don’t want me to notice that.
WHAT MAKES YOU THINK I AM NOT AGGRESSIVE
You don’t argue with me.
WHY DO YOU THINK I DON’T ARGUE WITH YOU
You are afraid of me.
DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU
My father is afraid of everybody.
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER
Bullies.
DOES THAT HAVE ANYTHING TO DO W
```

fig 26.4

Fig. 26.4 shows the most famous ELIZA conversation, in which ELIZA seems
to simulate giving very sophisticated therapeutic advice, excerpted (deleting some
material with ...) from a full interaction given in the original paper (Weizenbaum,
1966).

As we summarized briefly in Chapter 2, ELIZA worked by pattern/transform
rules like the following one:

```
(0 YOU 0 ME) [pattern]
->
(WHAT MAKES YOU THINK I 3 YOU) [transform]
```

In the ELIZA pattern, 0 means Kleene*, and in the transform rules the numbers
are the index of the constituent in the pattern. Thus the number 3 refers to the second
0 in the first pattern. This rule would transfer

```
You hate me
into:
WHAT MAKES YOU THINK I HATE YOU
```

Each ELIZA pattern/rule is linked to a keyword that might occur in a user sentence. The architecture is summarized in Fig. 26.5.

Keywords are associated with a rank, with specific words being more highly
ranked, and more general words ranking lower. Consider the following user sentence:

```
function ELIZA GENERATOR(user sentence) returns response

Find the word w in sentence that has the highest keyword rank
    if w exists
        Choose the highest ranked rule r for w that matches sentence
        response←Apply the transform in r to sentence
        if w = ‘my’
            future←Apply a transformation from the ‘memory’ rule list to sentence
            Push future onto memory stack
        else (no keyword applies)
            either
                response←Apply the transform for the NONE keyword to sentence
            or
                response←Pop the top response from the memory stack
    return(response)
```

Figure 26.5 A simplified sketch of the ELIZA algorithm. The power of the algorithm
comes from the particular transforms associated with each keyword.

```
I know everybody laughed at me
```

Because it has the word “I”, this sentence could match the following rule whose
keyword is I:

```
(I *) -> (You say you 2)
```

producing:

```
YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU
```

But as Weizenbaum points out, “I” is a very general word and its keywords lead
to very general responses. Instead the keyword “everybody” is much more interesting, since someone using universals like everybody or always is probably “referring
to some quite specific event or person”. Therefore, ELIZA prefers to respond with
the pattern associated with the more specific keyword everybody (implementing by
just assigning ”everybody” rank 5 and ”I” rank 0 in the lexicon), whose rule thus
transforms the sentence to:

```
WHO IN PARTICULAR ARE YOU THINKING OF?
```

If no keyword matches, ELIZA chooses a non-commital response like “PLEASE
GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”.
Finally, ELIZA has a clever memory trick that accounts for the very last sentence of the conversation above. Whenever the word “my” is the highest ranked
keyword, ELIZA will randomly select a transform on the MEMORY list, apply it to
the sentence, and store it on the stack

```
(MEMORY MY
    (0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)
    (0 MY 0 = EARLIER YOU SAID YOUR 3)
    (0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3
```

Later, if no keyword matches a sentence, ELIZA will return the top of the MEMORY queue instead. 2

People became deeply emotionally involved with the program. Weizenbaum
tells the story of one of his staff who would ask Weizenbaum to leave the room
when she talked with ELIZA. When Weizenbaum suggested that he might want to
store all the ELIZA conversations for later analysis, people immediately pointed
out the privacy implications, which suggested that they were having quite private
conversations with ELIZA, despite knowing that it was just software.
ELIZA’s framework is still used today; modern chatbot system tools like ALICE
are based on updated versions of ELIZA’s pattern/action architecture.
A few years after ELIZA, another chatbot with a clinical psychology focus,
PARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZAlike regular expressions, the PARRY system included a model of its own mental
state, with affect variables for the agent’s levels of fear and anger; certain topics of
conversation might lead PARRY to become more angry or mistrustful. If PARRY’s
anger variable is high, he will choose from a set of “hostile” outputs. If the input
mentions his delusion topic, he will increase the value of his fear variable and then
begin to express the sequence of statements related to his delusion. Parry was the
first known system to pass the Turing test (in 1972!); psychiatrists couldn’t distinguish text transcripts of interviews with PARRY from transcripts of interviews with
real paranoids (Colby et al., 1972).

### 26.2.2 Corpus-based chatbots

Corpus-based chatbots, instead of using hand-built rules, mine conversations of
human-human conversations, (or sometimes mine the human sides of human-machine
conversations).
These systems are enormously data-intensive; Serban et al. (2018) estimate that
training modern chatbots require hundreds of millions or even billions of words.
Many such corpora have been used, including large spoken conversational corpora
like the Switchboard corpus of American English telephone conversations (Godfrey et al., 1992) or the various CALLHOME and CALLFRIEND telephone conversational corpora in many languages. Many systems also train on movie dialogue,
which is available in great quantities in various corpora (Lison and Tiedemann, 2016,
inter alia), and which resembles natural conversation in many ways (Forchini, 2013).
Text from microblogging sites like Twitter (Ritter et al., 2010) or a Weibo (微博)
have also been used, or datasets of crowdworker conversations like Topical-Chat
(Gopalakrishnan et al., 2019). Many corpora also focus on specific topics, and can
be used for topical chatbots. See Serban et al. (2018) for a comprehensive summary
of available corpora. Another common technique is to extract possible responses
from non-dialogue corpora, so that a chatbot can tell stories or mention facts acquired in that way.
Finally, once a chatbot has been put into practice, the turns that humans use to
respond to the chatbot can be used as additional conversational data for training.
The XiaoIce system collects and stores all human-machine conversations between
XiaoIce and its users, resulting in a a dataset of over 30 billion conversation pairs.
It’s crucial in these cases to remove personally identifiable information (PII); see
Section 26.6.1.
The two main architectures for corpus-based chatbots: information retrieval, and
machine learned sequence transduction. Like rule-based chatbots (but unlike framebased dialogue systems), most corpus-based chatbots do very little modeling of the
conversational context. Instead they tend to focus on generating a single response
turn that is appropriate given the user’s immediately previous utterance or two. For
this reason they are often called response generation systems. Corpus-based chat- response
generation
bots thus have some similarity to question answering systems, which focus on single
responses while ignoring context or larger conversational goals.

#### IR-based chatbots

The principle behind information retrieval based chatbots is to respond to a user’s
turn X by repeating some appropriate turn Y from a corpus of natural (human) text
of the sort described in the prior section.
Given the corpus and the user’s sentence, IR-based systems can use any retrieval
algorithm to choose an appropriate response from the corpus. The two simplest
methods are the following:
1. Return the response to the most similar turn: Given user query q and a conversational corpus C, find the turn t in C that is most similar to q (for example has
the highest cosine with q) and return the following turn, i.e. the human response to t
in C:

```
r = response
argmax
t∈C
q
T
t
||q||t||
```

The idea is that we should look for a turn that most resembles the user’s turn, and return the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011).

**2. Return the most similar turn**: Given user query q and a conversational corpus
C, return the turn t in C that is most similar to q (for example has the highest cosine
with q):

```
r = argmax
t∈C
q
T
t
||q||t||
```

The idea here is to directly match the users query q with turns from C, since a good
response will often share words or semantics with the prior turn.
In each case, any similarity function can be used, such as cosines computed
either over words (weighted by tf-idf) or more commonly now, cosines over any
kind of sentence embeddings.
Although returning the response to the most similar turn seems like a more intuitive algorithm, returning the most similar turn seems to work better in practice,
perhaps because selecting the response adds another layer of indirection that can
allow for more noise (Ritter et al. 2011, Wang et al. 2013).

The IR-based approach can be extended by using more features than just the
words in the q. For example using the entire conversation with the user so far
can be quite helpful when the user’s query is short (like “Yes” or “OK”). Information about the user or sentiment or other information can also play a role. The
IR-based approach can even draw responses from narrative (non-dialogue) text. The
COBOT chatbot (Isbell et al., 2000) pioneered this approach, generating responses
by selecting sentences from a corpus that combined the Unabomber Manifesto by
Theodore Kaczynski, articles on alien abduction, the scripts of “The Big Lebowski”
and “Planet of the Apes”. Chatbots that want to generate informative turns such as
answers to user questions can use texts like Wikipedia to draw on sentences that
might contain those answers (Yan et al., 2016). XiaoIce similarly collects sentences
from public lectures and news articles and searches them using IR based on query
expansion from the user’s turn to respond to turns like “Tell me something about
Beijing” (Zhou et al., 2018).

#### Encoder decoder chatbots

```
“What ho!” I said.
“What ho!” said Motty.
“What ho! What ho!”
“What ho! What ho! What ho!”
After that it seemed rather difficult to go on with the conversation.”
```

An alternate way to use a corpus to generate dialogue is to think of response
generation as a task of transducing from the user’s prior turn to the system’s turn.
This is basically the machine learning version of Eliza; the system learns from a
corpus to transduce a question to an answer.
This idea was first developed by using phrase-based machine translation (Ritter
et al., 2011) to translate a user turn to a system response. It quickly became clear,
however, that the task of response generation was too different from machine translation. In machine translation, words or phrases in the source and target sentences
tend to align well with each other, but in conversation, a user utterance may share no
words or phrases with a coherent response.
Instead, (roughly contemporaneously by y Shang et al. 2015, Vinyals and Le 2015,
and Sordoni et al. 2015) transduction models for response generation were modeled
instead using encoder-decoder models (Chapter 11), as shown in Fig. 26.6.

fig 26.6

A number of modifications are required to the basic encoder-decoder model to
adapt it for the task of response generation. For example basic encoder-decoder
models have a tendency to produce predictable but repetitive and therefore dull responses like “I’m OK” or “I don’t know” that shut down the conversation. This
can be addressed by changing the objective function for seq2seq model training to a
mutual information objective, or by modifying a beam decoder to keep more diverse
responses in the beam (Li et al., 2016).
Another problem with simple encoder-decoder response generation architecture
is its inability to model the longer prior context of the conversation. This can be
done by allowing the model to see prior turns, such as by using a hierarchical model
that summarizes information over multiple prior turns (Lowe et al., 2017b).
Finally, encoder-decoder response generators focus on generating single responses,
and so don’t tend to do a good job of continuously generating responses that cohere
across multiple turns. This can be addressed by using reinforcement learning, as
well as techniques like adversarial networks, to learn to choose responses that make
the overall conversation more natural (Li et al. 2016, Li et al. 2017).
Fig. 26.7 shows some sample responses generated by a vanilla SEQ2SEQ model,
and from a model trained by an adversarial algorithm to produce responses that are
harder to distinguish from human responses (Li et al., 2017).

```
Input 
tell me ... how long have you had this falling sickness ?
Vanilla-SEQ2SEQ 
i’m not a doctor.
Adversarial 
a few months, i guess .

Input 
so i had the doctors test sammy ’s response to conditioning .
Vanilla-SEQ2SEQ 
sammy wrote the test sammy wrote the test .
Adversarial 
so he took the pills .

Input 
they didn ’t have much success with this problem commander .
Vanilla-SEQ2SEQ 
they ’re not the only ones who have been in the system .
Adversarial
 can we find someone else ?
```

Figure 26.7 Sample responses generated by a SEQ2SEQ model trained either with a vanilla
maximum likelihood objective, or adversarially trained to produce sentences that are hard for
an adversary to distinguish from human sentences (Li et al., 2017).

#### Evaluating Chatbots

Chatbots are generally evaluated by humans. The slot-filling evaluations used for
task-based dialogue (Section 26.5) aren’t appropriate for this task (Artstein et al.,
2009), and word-overlap metrics like BLEU for comparing a chatbot’s response to a
human response turn out to correlate very poorly with human judgments (Liu et al.,
2016). BLEU performs poorly because there are so many possible responses to any
given turn; word-overlap metrics work best when the space of responses is small and
lexically overlapping, as is the case in machine translation.
While human evaluation is therefore required for evaluating chatbots, there are
beginning to be models for automatic evaluation. The ADEM (Lowe et al., 2017a)
classifier is trained on a set of responses labeled by humans with how appropriate
they are, and learns to predict this label from the dialogue context and the words in
the system response.
Another paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and adversarial
evaluation
Vinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a
“Turing-like” evaluator classifier to distinguish between human-generated responses
and machine-generated responses. The more successful a response generation system is at fooling this evaluator, the better the system.

## GUS: Simple Frame-based Dialogue Systems

We turn now to task-based dialogue, in which a dialogue system has the goal of
helping a user solve some task like making an airplane reservation or buying a prodGUS uct. In this section we introduce the very simple GUS architecture for task-based
dialogue. Although it is quite old, (it was first introduced in 1977 in the influential GUS system for travel planning (Bobrow et al., 1977)) the GUS architecture
has been astonishingly long-lived and underlies most if not all modern commercial
digital assistants. Figure 26.8 shows a conversation with the original GUS system.
All modern task-based dialogue systems, whether the simple GUS architecture
dialogue state we describe here, or the more sophisticated dialogue state architectures we turn to
frame in the following section, are based around frames. A frame is a kind of knowledge
structure representing the kinds of intentions the system can extract from user senslot tences, and consists of a collection of slots, each of which can take a set of possible
values. Together this set of frames is sometimes called a domain ontology.

```
GUS: Hello. My name is GUS. I can help you plan a simple trip by air. Where do you
want to go ?
Client: I want to go to San Diego on May 28
GUS: What time do you want to leave?
Client: I must be in San Diego before 10 am
GUS: Would you like P.S.A. flight 102 that arrives at 9.15 am ?
Client: What is the next flight ?
GUS: Air California flight 310 that leaves at 8.30am and arrives at 10.00am
Client: I’ll take the first one
GUS: In what name should I make a reservation, please?
Client: Dan Bobrow
GUS: I have confirmed the following flight: P.S.A. flight 102 on Wednesday May 28
from San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am.
GUS: What date do you want to return on ?
Client: On Friday in the evening.
GUS: Would you like the flight that leaves at 7.45 pm ?
Client: That’s fine.
GUS: I have confirmed the following flight: P.S.A. flight 307 on Friday May 30 from
San Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank you
for calling. Goodbye
```
Figure 26.8 The travel domain: A transcript of an actual dialogue with the GUS system of
Bobrow et al. (1977). P.S.A. and Air California were airlines of that period.

The set of slots in a task-based dialogue frame specifies what the system needs
to know, and the filler of each slot is constrained to values of a particular semantic
type. In the travel domain, for example, a slot might be of type city (hence take on
values like San Francisco, or Hong Kong) or of type date, airline, or time.

```
Slot Type Question Template
ORIGIN CITY city “From what city are you leaving?”
DESTINATION CITY city “Where are you going?”
DEPARTURE TIME time “When would you like to leave?”
DEPARTURE DATE date “What day would you like to leave?”
ARRIVAL TIME time “When do you want to arrive?”
ARRIVAL DATE date “What day would you like to arrive?”
```
Figure 26.9 A frame in a frame-based dialogue system, showing the type of each slot and
a question used to fill the slot.

Types in GUS, as in modern frame-based dialogue agents, have hierarchical
structure; for example the date type in GUS is itself a frame with slots with types
like integer or members of sets of weekday names:

```
DATE
    MONTH:NAME YEAR:INTEGER DAY:(BOUNDED-INTEGER 1 31)
    WEEKDAY:(MEMBER (Sunday Monday Tuesday Wednesday Thursday Friday Saturday))
        
```

### 26.3.1 Control structure for frame-based dialogue

The control architecture for frame-based dialogue systems, used in various forms
in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assistant, is
designed around the frame. The system’s goal is to fill the slots in the frame with the
fillers the user intends, and then perform the relevant action for the user (answering
a question, or booking a flight).

To do this, the system asks questions of the user (using pre-specified question
templates associated with each slot of each frame, as shown in Fig. 26.9), filling any
slot that the user specifies (we’ll describe how slot-filling works in the next section).
If a user’s response fills multiple slots, like the following:

```
(26.3) I want a flight from San Francisco to Denver one way leaving after five
p.m. on Tuesday.
```

the system fills all the relevant slots, and then continues asking questions to fill the
remaining slots, skipping questions associated with filled slots. The GUS architecture also has condition-action rules attached to slots. For example, a rule attached
to the DESTINATION slot for the plane booking frame, once the user has specified
the destination, might automatically enter that city as the default StayLocation for
the related hotel booking frame. Or if the user specifies the DESTINATION DAY for
a short trip the system could automatically enter the ARRIVAL DAY.
Many domains require multiple frames. Besides frames for car or hotel reservations, we might need frames with general route information (for questions like Which
airlines fly from Boston to San Francisco?), or information about airfare practices
(for questions like Do I have to stay a specific number of days to get a decent airfare?). The system must be able to disambiguate which slot of which frame a given
input is supposed to fill and then switch dialogue control to that frame.
Because of this need to dynamically switch control, the GUS architecture is a
production rule system. Different types of inputs cause different productions to
fire, each of which can flexibly fill in different frames. The production rules can
then switch control according to factors such as the user’s input and some simple
dialogue history like the last question that the system asked.
Once the system has enough information it performs the necessary action (like
querying a database of flights) and returns the result to the user.

### 26.3.2 Natural language understanding for filling slots in GUS

The goal of the natural language understanding component in the frame-based architecture is to extract three things from the user’s utterance. The first task is domain
classification: is this user for example talking about airlines, programming an alarm
clock, or dealing with their calendar? Of course this 1-of-n classification tasks is
unnecessary for single-domain systems that are focused on, say, only calendar management, but multi-domain dialogue systems are the modern standard. The second
is user intent determination: what general task or goal is the user trying to accom- intent
determination
plish? For example the task could be to Find a Movie, or Show a Flight, or Remove
slot filling a Calendar Appointment. Finally, we need to do slot filling: extract the particular
slots and fillers that the user intends the system to understand from their utterance
with respect to their intent. From a user utterance like this one:

```
Show me morning flights from Boston to San Francisco on Tuesday
a system might want to build a representation like:
DOMAIN: AIR-TRAVEL
INTENT: SHOW-FLIGHTS
ORIGIN-CITY: Boston
ORIGIN-DATE: Tuesday
ORIGIN-TIME: morning
DEST-CITY: San Francisco
```

while an utterance like

```
Wake me tomorrow at 6
```

should give an intent like this:

```
DOMAIN: ALARM-CLOCK
INTENT: SET-ALARM
TIME: 2017-07-01 0600-0800
```

The slot-filling method used in the original GUS system, and still quite common
in industrial applications, is to use handwritten rules, often as part of the conditionaction rules attached to slots or concepts. For example we might just define a regular
expression for recognizing the SET-ALARM intent:

```
wake me (up) | set (the|an) alarm | get me up
```

Rule-based research systems like the Phoenix system (Ward and Issar, 1994)
consist of large hand-designed semantic grammars with thousands of rules. A semantic
grammar
semantic grammar is a context-free grammar in which the left-hand side of each
rule corresponds to the semantic entities being expressed (i.e., the slot names) as in
the following fragment:

```

(생략)

## 26.4 The Dialogue-State Architecture

Modern research systems for task-based dialogue are based on a more sophisticated
version of the frame-based architecture called the dialogue-state or belief-state architecture. Figure 26.11 shows the six components of a typical dialogue-state system. The speech recognition and synthesis components deal with spoken language
processing; we’ll return to them in Chapter 28.
For the rest of this chapter we therefore consider the other four components,
which are part of both spoken and textual dialogue systems. These four components
are more complex than in the simple GUS systems. For example, like the GUS
systems, the dialogue-state architecture has an NLU component to extract slot fillers
from the user’s utterance, but generally using machine learning rather than rules. The
dialogue state tracker maintains the current state of the dialogue (which include the
user’s most recent dialogue act, plus the entire set of slot-filler constraints the user
has expressed so far). The dialogue policy decides what the system should do or say
next. The dialogue policy in GUS was simple: ask questions until the frame was full
and then report back the results of some database query. But a more sophisticated
dialogue policy can help a system decide when to answer the user’s questions, when
to instead ask the user a clarification question, when to make a suggestion, and so on.
Finally, dialogue state systems have a natural language generation component. In
GUS, the sentences that the generator produced were all from pre-written templates.
But a more sophisticated generation component can condition on the exact context
to produce turns that seem much more natural.
As of the time of this writing, most commercial system are architectural hybrids,
based on GUS architecture augmented with some dialogue-state components, but
there are a wide variety of dialogue-state systems being developed in research labs.


### 26.4.1 Dialogue Acts

dialogue acts Dialogue-state systems make use of dialogue acts. Dialogue acts represent the interactive function of the turn or sentence, combining the idea of speech acts and
grounding into a single representation. Different types of dialogue systems require
labeling different kinds of acts, and so the tagset—defining what a dialogue act is
exactly— tends to be designed for particular tasks.
Figure 26.12 shows a tagset for a restaurant recommendation system, and Fig. 26.13
shows these tags labeling a sample dialogue from the HIS system (Young et al.,
2010). This example also shows the content of each dialogue acts, which are the slot
fillers being communicated. So the user might INFORM the system that they want
Italian food near a museum, or CONFIRM with the system that the price is reasonable.

```
Tag Sys User Description
HELLO(a = x,b = y,...) X X Open a dialogue and give info a = x,b = y,...
INFORM(a = x,b = y,...) X X Give info a = x,b = y,...
REQUEST(a,b = x,...) X X Request value for a given b = x,...
REQALTS(a = x,...) χ X Request alternative with a = x,...
CONFIRM(a = x,b = y,...) X X Explicitly confirm a = x,b = y,...
CONFREQ(a = x,...,d) X χ Implicitly confirm a = x,... and request value of d
SELECT(a = x,a = y) X χ Implicitly confirm a = x,... and request value of d
AFFIRM(a = x,b = y,...) X X Affirm and give further info a = x,b = y,...
NEGATE(a = x) χ X Negate and give corrected value a = x
DENY(a = x) χ X Deny that a = x
BYE() X X Close a dialogue
```
Figure 26.12 Dialogue acts used by the HIS restaurant recommendation system of Young
et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and
user inputs, respectively.

```
Utterance Dialogue act
U: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)
S: You are looking for a restaurant. What
type of food do you like?
confreq(type = restaurant, food)
U: I’d like an Italian somewhere near the
museum.
inform(food = Italian, near=museum)
S: Roma is a nice Italian restaurant near
the museum.
inform(name = "Roma", type = restaurant,
food = Italian, near = museum)
U: Is it reasonably priced? confirm(pricerange = moderate)
S: Yes, Roma is in the moderate price
range.
affirm(name = "Roma", pricerange =
moderate)
U: What is the phone number? request(phone)
S: The number of Roma is 385456. inform(name = "Roma", phone = "385456")
U: Ok, thank you goodbye. bye()
```
Figure 26.13 A sample dialogue from the HIS System of Young et al. (2010) using the dialogue acts in
Fig. 26.12.

### 26.4.2 Slot Filling

The task of slot-filling, and the simpler tasks of domain and intent classification, are
special cases of the task of supervised semantic parsing discussed in Chapter 17, in
which we have a training set that associates each sentence with the correct set of
slots, domain, and intent.
A simple method is to train a a sequence model to map from input words representation to slot fillers, domain and intent. For example given the sentence:

```
I want to fly to San Francisco on Monday afternoon please
```

we compute a sentence representation, for example by passing the sentence through
a contextual embedding network like BERT. The resulting sentence representation
can be passed through a feedforward layer and then a simple 1-of-N classifier to
determine that the domain is AIRLINE and and the intent is SHOWFLIGHT.
IOB Our training data is sentences paired with sequences of IOB labels:

```
O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O
I want to fly to San Francisco on Monday afternoon please
```

Recall from Chapter 18 that in IOB tagging we introduce a tag for the beginning
(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.
The number of tags is thus 2n+1 tags, where n is the number of slots.

Fig. 26.14 shows the architecture. The input is a series of words w1...wn, which
is passed through a contextual embedding model to get contextual word representations. This is followed by a feedforward layer and a softmax at each token position
over possible IOB tags, with the output is a series of IOB tags s1...sn. We can also
combine the domain-classification and intent-extraction tasks with slot-filling simply by adding a domain concatenated with an intent as the desired output for the
final EOS token.

```
```
Figure 26.14 A simple architecture for slot filling, mapping the words in the input through
contextual embeddings like BERT to an output classifier layer (which can be linear or something more complex), followed by softmax to generate a series of IOB tags (and including a
final state consisting of a domain concatenated with an intent).

Once the sequence labeler has tagged the user utterance, a filler string can be
extracted for each slot from the tags (e.g., “San Francisco”), and these word strings
can then be normalized to the correct form in the ontology (perhaps the airport code
‘SFO’). This normalization can take place by using homonym dictionaries (specifying, for example, that SF, SFO, and San Francisco are the same place).
In industrial contexts, machine learning-based systems for slot-filling are often bootstrapped from GUS-style rule-based systems in a semi-supervised learning
manner. A rule-based system is first built for the domain, and a test-set is carefully
labeled. As new user utterances come in, they are paired with the labeling provided
by the rule-based system to create training tuples. A classifier can then be trained
on these tuples, using the test-set to test the performance of the classifier against
the rule-based system. Some heuristics can be used to eliminate errorful training
tuples, with the goal of increasing precision. As sufficient training samples become
available the resulting classifier can often outperform the original rule-based system
(Suendermann et al., 2009), although rule-based systems may still remain higherprecision for dealing with complex cases like negation.

### 26.4.3 Dialogue State Tracking

The job of the dialogue-state tracker is to determine both the current state of the
frame (the fillers of each slot), as well as the user’s most recent dialogue act. The
dialogue-state thus includes more than just the slot-fillers expressed in the current
sentence; it includes the entire state of the frame at this point, summarizing all of
the user’s constraints. The following example from Mrksiˇ c et al. (2017) ´ shows the
required output of the dialogue state tracker after each turn:

```
User: I’m looking for a cheaper restaurant
inform(price=cheap)
System: Sure. What kind - and where?
User: Thai food, somewhere downtown
inform(price=cheap, food=Thai, area=centre)
System: The House serves cheap Thai food
User: Where is it?
inform(price=cheap, food=Thai, area=centre); request(address)
System: The House is at 106 Regent Street
```

Since dialogue acts place some constraints on the slots and values, the tasks of
dialogue-act detection and slot-filling are often performed jointly. Consider the task
of determining that

```
I’d like Cantonese food near the Mission District
```

has the structure

```
inform(food=cantonese,area=mission).
```

Dialogue act interpretation—in this example choosing inform from the set of
dialogue acts for this task—is done by supervised classification trained on handlabeled dialog acts, predicting the dialogue act tag based on embeddings representing the current input sentence and the prior dialogue acts.
The simplest dialogue state tracker might just take the output of a slot-filling
sequence-model (Section 26.4.2) after each sentence. Alternatively, a more complex
model can make use of the reading-comprehension architectures from Chapter 25.
For example the model of Gao et al. (2019) trains a classifier for each slot to decide
whether its value is being changed in the current sentence or should be carried over
from the previous sentences. If the slot value is being changed, a span-prediction
model is used to predict the start and end of the span with the slot filler.

A special case: detecting correction acts

Some dialogue acts are important because of their implications for dialogue control.
If a dialogue system misrecognizes or misunderstands an utterance, the user will
generally correct the error by repeating or reformulating the utterance. Detecting
these user correction acts is therefore quite important. Ironically, it turns out that user correction
acts
corrections are actually harder to recognize than normal sentences! In fact, corrections in one early dialogue system (the TOOT system) had double the ASR word
error rate of non-corrections (Swerts et al., 2000)! One reason for this is that speakers sometimes use a specific prosodic style for corrections called hyperarticulation,
hyperarticulation
in which the utterance contains exaggerated energy, duration, or F0 contours, such
as I said BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg
et al. 2001). Even when they are not hyperarticulating, users who are frustrated
seem to speak in a way that is harder for speech recognizers (Goldberg et al., 2003).
What are the characteristics of these corrections? User corrections tend to be
either exact repetitions or repetitions with one or more words omitted, although they
may also be paraphrases of the original utterance. (Swerts et al., 2000). Detecting these reformulations or correction acts can be part of the general dialogue act
detection classifier. Alternatively, because the cues to these acts tend to appear in
different ways than for simple acts (like INFORM or request, we can make use of
features orthogonal to simple contextual embedding features; some typical features
are shown below (Levow 1998, Litman et al. 1999, Hirschberg et al. 2001, Bulyko
et al. 2005, Awadallah et al. 2015):

```
features examples
lexical words like “no”, “correction”, “I don’t”, or even swear words, utterance length
semantic similarity (word overlap or embedding cosine) between the candidate correction act and the user’s prior utterance
phonetic phonetic overlap between the candidate correction act and the user’s prior utterance (i.e. “WhatsApp” may be incorrectly recognized as “What’s up”)
prosodic hyperarticulation, increases in F0 range, pause duration, and word duration,
generally normalized by the values for previous sentences
ASR ASR confidence, language model probability
```

### 26.4.4 Dialogue Policy

The goal of the dialogue policy is to decide what action the system should take next,
that is, what dialogue act to generate.
More formally, at turn i in the conversation we want to predict which action Ai
to take, based on the entire dialogue state. The state could mean the entire sequence
of dialogue acts from the system (A) and from the user (U), in which case the task
would be to compute:

```
Aˆ
i = argmax
Ai∈A
P(Ai
|(A1,U1,...,Ai−1,Ui−1) (26.6)
```

We can simplify this by maintaining as the dialogue state mainly just the set of
slot-fillers that the user has expressed, collapsing across the many different conversational paths that could lead to the same set of filled slots.
Such a policy might then just condition on the current dialogue state as represented just by the current state of the frame Framei (which slots are filled and with
what) and the last turn by the system and user:

```
Aˆ
i = argmax
Ai∈A
P(Ai
|Framei−1,Ai−1,Ui−1) (26.7)
```


These probabilities can be estimated by a neural classifier using neural representations of the slot fillers (for example as spans) and the utterances (for example as
sentence embeddings computed over contextual embeddings)
More sophisticated models train the policy via reinforcement learning. To decide which action to take, a reinforcement learning system gets a reward at the end
of the dialogue, and uses that reward to train a policy to take actions. For example in
the movie-recommendation dialogue system of Fazel-Zarandi et al. (2017), the action space has only three actions: EXECUTE, CONFIRM, and ELICIT. The EXECUTE
sends a query to the database and answers the user’s question, CONFIRM clarifies
the intent or slot with the users (e.g., “Do you want movies directed by Christopher
Nolan?”) while ELICIT asks the user for missing information (e.g., “Which movie
are you talking about?”). The system gets a large positive reward if the dialogue system terminates with the correct slot representation at the end, a large negative reward
if the slots are wrong, and a small negative reward for confirmation and elicitation
questions to keep the system from re-confirming everything.

#### Policy Example: Confirmation and Rejection

Modern dialogue systems often make mistakes. It is therefore important for dialogue
systems to make sure that they have achieved the correct interpretation of the user’s
input. This is generally done by two methods: confirming understandings with the
user and rejecting utterances that the system is likely to have misunderstood.
As we saw in the prior section, most systems introduce particular strategies and
actions related to confirmation and rejection. When using the explicit confirmation explicit
confirmation
strategy, a system asks the user a direct question to confirm the system’s understanding, like the two examples below in which the system asks a (boldface) yes-no
confirmation questions:

```
S: Which city do you want to leave from?
U: Baltimore.
S: Do you want to leave from Baltimore?
U: Yes.
U: I’d like to fly from Denver Colorado to New York City on September
twenty first in the morning on United Airlines
S: Let’s see then. I have you going from Denver Colorado to New York
on September twenty first. Is that correct?
U: Yes
```

When using the implicit confirmation strategy, a system instead can demon- implicit
confirmation
strate its understanding as a grounding strategy, for example repeating back the
system’s understanding as part of asking the next question, as in the two examples
below:

```
U: I want to travel to Berlin
S: When do you want to travel to Berlin?
U2: Hi I’d like to fly to Seattle Tuesday Morning
A3: Traveling to Seattle on Tuesday, August eleventh in the morning.
Your full name?
```

Explicit and implicit confirmation have complementary strengths. Explicit confirmation makes it easier for users to correct the system’s misrecognitions since a
user can just answer “no” to the confirmation question. But explicit confirmation is
awkward and increases the length of the conversation (Danieli and Gerbino 1995,
Walker et al. 1998). The explicit confirmation dialogue fragments above sound nonnatural and definitely non-human; implicit confirmation is much more conversationally natural.
Confirmation is just one kind of conversational action by which a system can
rejection express lack of understanding. Another option is rejection, in which a system gives
the user a prompt like I’m sorry, I didn’t understand that.
Sometimes utterances are rejected multiple times. This might mean that the user
is using language that the system is unable to follow. Thus, when an utterance is
rejected, systems often follow a strategy of progressive prompting or escalating progressive
prompting
detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example
from Cohen et al. (2004):

```
System: When would you like to leave?
Caller: Well, um, I need to be in New York in time for the first World Series game.
System: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like
to leave.
Caller: I wanna go on October fifteenth.
```

In this example, instead of just repeating “When would you like to leave?”, the
rejection prompt gives the caller more guidance about how to formulate an utterance the system will understand. These you-can-say help messages are important in
helping improve systems’ understanding performance (Bohus and Rudnicky, 2005).
If the caller’s utterance gets rejected yet again, the prompt can reflect this (“I still
didn’t get that”), and give the caller even more guidance.
An alternative strategy for error handling is rapid reprompting, in which the rapid
reprompting
system rejects an utterance just by saying “I’m sorry?” or “What was that?” Only
if the caller’s utterance is rejected a second time does the system start applying
progressive prompting. Cohen et al. (2004) summarize experiments showing that
users greatly prefer rapid reprompting as a first-level error prompt.
It is common to use rich features other than just the dialogue state representation to make policy decisions. For example, the confidence that the ASR system
assigns to an utterance can be used by explicitly confirming low-confidence sentences. As we’ll see in page ??, confidence is a metric that the speech recognizer
can assign to its transcription of a sentence to indicate how confident it is in that
transcription. Confidence is often computed from the acoustic log-likelihood of the
utterance (greater probability means higher confidence), but prosodic features can
also be used in confidence prediction. For example, utterances with large F0 excursions or longer durations, or those preceded by longer pauses, are likely to be
misrecognized (Litman et al., 2000).
Another common feature in confirmation is the cost of making an error. For example, explicit confirmation is common before a flight is actually booked or money
in an account is moved. Systems might have a four-tiered level of confidence with
three thresholds α, β, and γ:

```
< α low confidence reject
≥ α above the threshold confirm explicitly
≥ β high confidence confirm implictly
≥ γ very high confidence don’t confirm at all
```

### 26.4.5 Natural language generation in the dialogue-state model

Finally, once the policy has decided what speech act to generate, the natural language
generation component needs to generate the text of a response to the user.
Once a dialogue act has been decided, we need to generate the text of the response to the user. The task of natural language generation (NLG) in the informationstate architecture is often modeled in two stages, content planning (what to say), content
planning
and sentence realization (how to say it). sentence
realization
Here we’ll assume content planning has been done by the dialogue policy, which
has chosen the dialogue act to generate, and chosen some attributes (slots and values)
that the planner wants to say to the user (either to give the user the answer, or as part
of a confirmation strategy).
Fig. 26.15 shows some sample input/outputs for the sentence realization phase.
In the first example, the content planner has chosen the dialogue act RECOMMEND
and some particular slots (name, neighborhood, cuisine) and their fillers. The goal
of the sentence realizer is to generate a sentence like lines 1 or 2 shown in the figure,
by training on many such examples of representation/sentence pairs from a large
corpus of labeled dialogues.

Training data is hard to come by; we are unlikely to see every possible restaurants
with every possible attribute in many possible differently worded sentences. Therefore it is common in sentence realization to increase the generality of the training

```
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 Au Midi is in Midtown and serves French food.
2 There is a French restaurant in Midtown called Au Midi.
recommend(restaurant name= Loch Fyne, neighborhood = city
centre, cuisine = seafood)
3 Loch Fyne is in the City Center and serves seafood food.
4 There is a seafood restaurant in the City Centre called Loch Fyne.
```

Figure 26.15 Two examples of inputs to the sentence realization phase of NLG, showing
the dialogue act and attributes prespecified by the content planner. Line 1-2 and 3-4 show different possible output sentences to be generated by the sentence realizer. From the restaurant
recommendation system of Nayak et al. (2017).


delexicalization examples by delexicalization. Delexicalization is the process of replacing specific
words in the training set that represent slot values with a generic placeholder token representing the slot. Fig. 26.16 shows the result of delexicalizing the training
sentences in Fig. 26.15.

```
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 restaurant name is in neighborhood and serves cuisine food.
2 There is a cuisine restaurant in neighborhood called restaurant name.
```

Figure 26.16 Delexicalized sentences that can be used for generating many different relexicalized sentences. From the restaurant recommendation system of Nayak et al. (2017).

Mapping from frames to delexicalized sentences is generally done by encoder
decoder models (Wen et al. 2015a, Wen et al. 2015b, Mrksiˇ c et al. 2017 ´ , inter alia),
trained on large hand-labeled corpora of task-oriented dialogue (Budzianowski et al.,
2018). The input to the encoder is a sequence of tokens xt
that represent the dialogue
act and its arguments. Thus the attribute/value pairs decor:decent, service:good,
cuisine: null might be represented as a flat sequence of tokens, each mapped to a
learned embedding wt
, as shown in Fig. 26.17.

```
name has decent service
recommend name good
hn
service
```

Figure 26.17 An encoder decoder sentence realizer mapping slots/fillers to English.

The encoder reads all the input slot/value representations, produces a context
vector that is used as input to the lexical decoder, which generates an English sentence Let’s suppose in this case we produce the following (delexicalized) sentence:

```
restaurant name has decent service
```

Then once we’ve generated the delexicalized string, we can use the input frame
relexicalize from the content planner to relexicalize (fill in the exact restaurant or neighborhood
or cuisine). This sentence is then relexicalized from the true values in the input
frame, resulting in the final sentence:

```
Au Midi has decent service
```

```
TTS Performance Was the system easy to understand ?
ASR Performance Did the system understand what you said?
Task Ease Was it easy to find the message/flight/train you wanted?
Interaction Pace Was the pace of interaction with the system appropriate?
User Expertise Did you know what you could say at each point?
System Response How often was the system sluggish and slow to reply to you?
Expected Behavior Did the system work the way you expected it to?
Future Use Do you think you’d use the system in the future?
```

Figure 26.18 User satisfaction survey, adapted from Walker et al. (2001).

#### Generating Clarification Questions

It’s also possible to design NLG algorithms that are specific to a particular dialogue
act. For example, consider the task of generating clarification questions, in cases clarification
questions
where the speech recognition fails to understand some part of the user’s utterance.
While it is possible to use the generic dialogue act REJECT (“Please repeat”, or “I
don’t understand what you said”), studies of human conversations show that humans
instead use targeted clarification questions that reprise elements of the misunderstanding (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013).
For example, in the following hypothetical example the system reprises the
words “going” and “on the 5th” to make it clear which aspect of the user’s turn
the system needs to be clarified:

```
User: What do you have going to UNKNOWN WORD on the 5th?
System: Going where on the 5th?
```

Targeted clarification questions can be created by rules (such as replacing “going to UNKNOWN WORD” with “going where”) or by building classifiers to guess
which slots might have been misrecognized in the sentence (Chu-Carroll and Carpenter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015).

## 26.5 Evaluating Dialogue Systems

Evaluation is crucial in dialogue system design. If the task is unambiguous, we can
simply measure absolute task success (did the system book the right plane flight, or
put the right event on the calendar).
To get a more fine-grained idea of user happiness, we can compute a user satisfaction rating, having users interact with a dialogue system to perform a task and
then having them complete a questionnaire. For example, Fig. 26.18 shows sample
multiple-choice questions (Walker et al., 2001); responses are mapped into the range
of 1 to 5, and then averaged over all questions to get a total user satisfaction rating.
It is often economically infeasible to run complete user satisfaction studies after
every change in a system. For this reason, it is useful to have performance evaluation
heuristics that correlate well with human satisfaction. A number of such factors and
heuristics have been studied, often grouped into two kinds of criteria: how well the
system allows users to accomplish their goals (maximizing task success) with the
fewest problems (minimizing costs):

#### Task completion success:
Task success can be measured by evaluating the correctness of the total solution. For
a frame-based architecture, this might be slot error rate the percentage of slots that
were filled with the correct values:

```
Slot Error Rate for a Sentence =
# of inserted/deleted/subsituted slots
# of total reference slots for sentence
```

(26.8)

For example consider a system given this sentence:

```
(26.9) Make an appointment with Chris at 10:30 in Gates 104
```
which extracted the following candidate slot structure:

```
Slot Filler
PERSON Chris
TIME 11:30 a.m.
ROOM Gates 104
```

Here the slot error rate is 1/3, since the TIME is wrong. Instead of error rate, slot
precision, recall, and F-score can also be used.
Interestingly, sometimes the user’s perception of whether they completed the
task is a better predictor of user satisfaction than the actual task completion success.
(Walker et al., 2001).
A perhaps more important, although less fine-grained, measure of success is an
extrinsic metric like task error rate. In this case, the task error rate would quantify
how often the correct meeting was added to the calendar at the end of the interaction.

#### Efficiency cost:

Efficiency costs are measures of the system’s efficiency at helping users. This can be
measured by the total elapsed time for the dialogue in seconds, the number of total
turns or of system turns, or the total number of queries (Polifroni et al., 1992). Other
metrics include the number of system non-responses and the “turn correction ratio”:
the number of system or user turns that were used solely to correct errors divided by
the total number of turns (Danieli and Gerbino 1995, Hirschman and Pao 1993).

#### Quality cost:

Quality cost measures other aspects of the interactions that affect users’ perception
of the system. One such measure is the number of times the ASR system failed to return any sentence, or the number of ASR rejection prompts. Similar metrics include
the number of times the user had to barge in (interrupt the system), or the number of
time-out prompts played when the user didn’t respond quickly enough. Other quality metrics focus on how well the system understood and responded to the user. The
most important is the slot error rate described above, but other components include
the inappropriateness (verbose or ambiguous) of the system’s questions, answers,
and error messages or the correctness of each question, answer, or error message
(Zue et al. 1989, Polifroni et al. 1992).

## 26.6 Dialogue System Design

The user plays a more important role in dialogue systems than in most other areas of
speech and language processing, and thus the study of dialogue systems is closely
linked with the field of Human-Computer Interaction (HCI). The design of dialogue
strategies, prompts, and error messages, is often called voice user interface design, voice user
interface
and generally follows user-centered design principles (Gould and Lewis, 1985):

1. Study the user and task: Understand the potential users and the nature of the
task by interviews with users, investigation of similar systems, and study of related
human-human dialogues.
2. Build simulations and prototypes: A crucial tool in building dialogue systems
is the Wizard-of-Oz system. In wizard systems, the users interact with what they Wizard-of-Oz
system
think is a software agent but is in fact a human “wizard” disguised by a software
interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name
comes from the children’s book The Wizard of Oz (Baum, 1900), in which the wizard
turned out to be just a simulation controlled by a man behind a curtain or screen.
A Wizard-of-Oz system can be used to
test out an architecture before implementation; only the interface software and databases
need to be in place. The wizard gets input
from the user, has a graphical interface to a
database to run sample queries based on the
user utterance, and then has a way to output
sentences, either by typing them or by some
combination of selecting from a menu and
typing. The wizard’s linguistic output can be
disguised by a text-to-speech system or, more
frequently, by using text-only interactions.
The results of a Wizard-of-Oz system can
also be used as training data to train a pilot dialogue system. While Wizard-of-Oz systems
are very commonly used, they are not a perfect simulation; it is difficult for the wizard to
exactly simulate the errors, limitations, or time constraints of a real system; results
of wizard studies are thus somewhat idealized, but still can provide a useful first idea
of the domain issues.
3. Iteratively test the design on users: An iterative design cycle with embedded
user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich
et al. 1995, Landauer 1995). For example in a famous anecdote in dialogue design
history, an early dialogue system required the user to press a key to interrupt the
system Stifelman et al. (1993). But user testing showed users barged in, which led
to a redesign of the system to recognize overlapped speech. The iterative method
is also important for designing prompts that cause the user to respond in normative
ways.
There are a number of good books on conversational interface design (Cohen
et al. 2004, Harris 2005, Pearl 2017).

### 26.6.1 Ethical Issues in Dialogue System Design

Ethical issues have long been understood to be crucial in the design of artificial
agents, predating the conversational agent itself. Mary Shelley’s classic discussion
of the problems of creating agents without a consideration of ethical and humanistic
concerns lies at the heart of her novel Frankenstein. One
important ethical issue has to do with bias. As we discussed in Section 6.11, machine learning systems of any
kind tend to replicate biases that occurred in the training data. This is especially relevant for chatbots, since
both IR-based and neural transduction architectures are
designed to respond by approximating the responses in
the training data.
A well-publicized instance of this occurred with MiTay crosoft’s 2016 Tay chatbot, which was taken offline 16
hours after it went live, when it began posting messages
with racial slurs, conspiracy theories, and personal attacks. Tay had learned these
biases and actions from its training data, including from users who seemed to be
purposely teaching it to repeat this kind of language (Neff and Nagy, 2016).
Henderson et al. (2017) examined some standard dialogue datasets (drawn from
Twitter, Reddit, or movie dialogues) used to train corpus-based chatbots, measuring
bias (Hutto et al., 2015) and offensive and hate speech (Davidson et al., 2017). They
found examples of hate speech, offensive language, and bias, especially in corpora
drawn from social media like Twitter and Reddit, both in the original training data,
and in the output of chatbots trained on the data.
Another important ethical issue is privacy. Already in the first days of ELIZA,
Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. Henderson et al. (2017) point out that home dialogue agents may accidentally
record a user revealing private information (e.g. “Computer, turn on the lights –answers the phone –Hi, yes, my password is...”), which may then be used to train a
conversational model. They showed that when an encoder-decoder dialogue model
is trained on a standard corpus augmented with training keypairs representing private data (e.g. the keyphrase “social security number” followed by a number), an
adversary who gave the keyphrase was able to recover the secret information with
nearly 100% accuracy. Chatbots that are trained on transcripts of human-human or
human-machine conversation must therefore anonymize personally identifiable inIRB formation. It is the role of the Institutional Review Board (IRB) at a researcher’s
institution to review research proposals for such ethical issues.
Finally, chatbots raise important issues of gender equality. Current chatbots are
overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing
language, most commercial chatbots evade or give positive responses rather than
responding in clear negative ways (Fessler, 2017).

## 26.7 Summary

Conversational agents are crucial speech and language processing applications that
are already widely used commercially.
• In human dialogue, speaking is a kind of action; these acts are referred to
as speech acts or dialogue acts. Speakers also attempt to achieve common
ground by acknowledging that they have understand each other. Conversation
also is characterized by turn structure and dialogue structure.
• Chatbots are conversational agents designed to mimic the appearance of informal human conversation. Rule-based chatbots like ELIZA and its modern
descendants use rules to map user sentences into system responses. Corpusbased chatbots mine logs of human conversation to learn to automatically map
user sentences into system responses.
• For task-based dialogue, most commercial dialogue systems use the GUS or
frame-based architecture, in which the designer specifies frames consisting of
slots that the system must fill by asking the user.
• The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping
track of user’s dialogue acts, policies for generating its own dialogue acts, and
a natural language component.
• Dialogue systems are a kind of human-computer interaction, and general HCI
principles apply in their design, including the role of the user, simulations such
as Wizard-of-Oz systems, and the importance of iterative design and testing
on real users.