# CH26 Dialogue Systems and Chatbots

The literature of the fantastic abounds in inanimate objects magically endowed with sentience and the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s Frankenstein, there is something deeply moving about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so life like that he tapped it on the knee and commanded it to speak. Perhaps this shouldn’t be surprising. Language is the mark conversation of humanity and sentience, and conversation or dialogue dialogue is the most fundamental and specially privileged arena of language. It is the first kind of language we learn as children, and for most of us, it is the kind of language we most commonly indulge in, whether we are ordering curry for lunch or buying spinach, participating in business meetings or talking with our families, booking airline flights or complaining about the weather.

(초벌 유지)
무생물에 대한 환상적인 문헌은 마술과 선의의 선물로 마술을 부여했습니다. 오비드의 피그말리온 동상에서 메리 셸리의 프랑켄슈타인에 이르기까지 무언가를 만들고 대화를 나누는 데 깊은 감동이 있습니다. 전설에 따르면 미켈란젤로는 조각 모세를 마친 후에 무릎을 꿇고 말하기를 마치는 것처럼 인생을 그렇게 생각했습니다. 아마도 이것은 놀라운 일이 아닙니다. 언어는 인류와 관용의 마크 대화이며, 대화 또는 대화 대화는 가장 기본적이고 특권이있는 언어 영역입니다. 점심은 카레를 주문하든 시금치를 사 든가, 비즈니스 미팅에 참여하거나 가족과 대화를 나눌 때 가장 많이 사용하는 언어입니다. , 항공사 항공편 예약 또는 날씨에 대한 불만.

This chapter introduces the fundamental algorithms of conversational agents, conversational agent dialogue system or dialogue systems. These programs communicate with users in natural language(text, speech, or both), and fall into two classes. Task-oriented dialogue agents use conversation with users to help complete tasks. Dialogue agents in digital assistants(Siri, Alexa, Google Now/Home, Cortana, etc.), give directions, control appliances, find restaurants, or make calls. Conversational agents can answer questions on corporate websites, interface with robots, and even be used for social good: DoNotPay is a "robot lawyer" that helps people challenge incorrect parking fines, apply for emergency housing, or claim asylum if they are refugees. By contrast, chatbots are systems designed for extended conversations, set up to mimic the unstructured conversations or ‘chats’ characteristic of human-human interaction, mainly for entertainment, but also for practical purposes like making task-oriented agents more natural. In Section 26.2 we’ll discuss the three major chatbot architectures : rulebased systems, information retrieval systems, and encoder-decoder models. In Section 26.3 we turn to task-oriented agents, introducing the frame-based architecture (the GUS architecture) that underlies most modern task-based systems.

이 장에서는 대화형 에이전트, 대화형 에이전트 대화 시스템 혹은 대화 시스템의 기본 알고리즘을 소개합니다. 이 프로그램은 자연 언어(텍스트, 음성 또는 둘 다)로 사용자와 통신하며 두 가지 클래스로 나뉩니다. 작업 지향 대화형 에이전트는 사용자와의 대화로 작업을 완료합니다. 디지털 비서 (Siri, Alexa, Google Now / Home, Cortana 등)의 대화 상담원은 길 찾기, 기기 제어, 식당 찾기 또는 전화 걸기 등을 수행합니다. 대화 상담원은 회사 웹 사이트에 대한 질문에 답변하고 로봇과의 인터페이스를 제공하며 사회적 이익을 위해 사용될 수도 있습니다. DoNotPay는 사람들이 잘못된 주차 벌금에 이의를 제기하거나 긴급 주택을 신청하거나 난민 인 경우 망명을 신청하는 데 도움이되는 "로봇 변호사"입니다. 반대로 챗봇은 확장 된 대화를 위해 고안된 시스템으로, 주로 엔터테인먼트 뿐만 아니라 업무 지향적인 상담원을보다 자연스럽게 만드는 것과 같은 실제적인 목적을 위해 구조화되지 않은 대화 또는 인간과 인간의 상호 작용의 '채팅'을 모방하도록 설정되었습니다. 26.2 절에서는 규칙 기반 시스템, 정보 검색 시스템 및 인코더 디코더 모델의 세 가지 주요 챗봇 아키텍처에 대해 설명합니다. 26.3 절에서 우리는 작업 지향 에이전트로 돌아가서 가장 현대적인 작업 기반 시스템의 기본이되는 프레임 기반 아키텍처 (GUS 아키텍처)를 소개합니다.

##  Properties of Human Conversation

Conversation between humans is an intricate and complex joint activity. Before we attempt to design a conversational agent to converse with humans, it is crucial to understand something about how humans converse with each other. Consider some of the phenomena that occur in the conversation between a human travel agent and a human client excerpted in Fig. 26.1.

인간 간의 대화는 복잡하고 복잡한 공동 활동입니다. 인간과 대화 할 대화형 에이전트를 디자인하기 전에 인간이 서로 대화하는 방법에 대해 이해하는 것이 중요합니다. 그림 26.1에서 발췌 한 인간 여행사와 인간 고객 간의 대화에서 발생하는 상황을 살펴보십시오.

```
C1: . . . I need to travel in May.
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
A2: And you’re flying into what city?
C3: Seattle.
A3: And what time would you like to leave Pittsburgh?
C4: Uh hmm I don’t think there’s many options for non-stop.
A4: Right. There’s three non-stops today.
C5: What are they?
A5: The first one departs PGH at 10:00am arrives Seattle at 12:05 their time. The second flight departs PGH at 5:55pm, arrives Seattle at 8pm. And the last
flight departs PGH at 8:15pm arrives Seattle at 10:28pm.
C6: OK I’ll take the 5ish flight on the night before on the 11th.
A6: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air flight 115.
C7: OK.
A7: And you said returning on May 15th?
C8: Uh, yeah, at the end of the day.
A8: OK. There’s #two non-stops . . . #
C9: #Act. . . actually #, what day of the week is the 15th?
A9: It’s a Friday.
C10: Uh hmm. I would consider staying there an extra day til Sunday.
A10: OK. . . OK. On Sunday I have . . .
```

### Turns

A dialogue is a sequence of turns (A1, B1, A2, and so on), each a single contribution to the dialogue (as if in a game: I take a turn, then you take a turn, then me, and so on). A turn can consist of a sentence (like C1), although it might be as short as a single word (C7) or as long as multiple sentences (A5).

대화는 턴(A1, B1, A2 등)의 연속이며, 대화 중 한번씩의 활동 입니다.(게임에서 내가 한번, 그리고 상대방이 한번 그다음 나, 이런식으로). 턴은 한 단어(C7)만큼 짧거나 여러 문장 (A5)만큼 길수도 있고 단일 문장(예 : C1)으로 구성 될 수 있습니다.

Turn structure has important implications for spoken dialogue. A system has to know when to stop talking; the client interrupts (in A8 and C9), so the system must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are able to (most of the time) detect when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This task endpointing called endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns.

턴 구조는 음성 대화에 중요한 영향을 미칩니다. 시스템은 언제 말을 멈추어야하는지 알아야 합니다. (A8 및 C9에서) 클라이언트가 대화를 중단 시켰을 때 시스템은 대화를 중지해야하며 사용자가 정정 할 수 있음을 알아야합니다. 시스템은 언제 이야기를 시작할 지 알아야합니다. 예를 들어, 대부분의 대화에서, 화자는 상대방이 대화를 마치려고 할 때 길게 멈추지 않더라도 감지 할 수 있기 때문에 상대방이 말을 끝낸 후 거의 즉시 자신의 턴을 시작합니다. 음성 대화 시스템은 사용자의 발화 여부를 감지하여 발화를 처리하고 응답 할 수 있어야합니다. 엔드 포인팅 또는 엔드 포인트 감지라고하는 이 대화 종료 감지 작업은 소음 이나 사람들이 종종 턴 중간에 말을 멈출 수 있기 때문에 상당히 어려울 수 있습니다.

A key insight into conversation—due originally to the philosopher Wittgenstein (1953) but worked out more fully by Austin (1962)—is that each utterance in a dialogue is a kind of action being performed by the speaker. These actions are commonly speech acts called speech acts or dialog acts: here’s one taxonomy consisting of 4 major classes (Bach and Harnish, 1979):

철학자 Wittgenstein (1953)에 의해 이야기되었고 오스틴 (1962)에 의해 더욱 완벽하게 정의 된 대화에 대한 주요 통찰은 대화의 각 발화가 화자에 의해 수행되는 일종의 행동이라는 것입니다. 이러한 행동은 일반적으로 발화 행위 또는 대화 행위라고하는 발화 행위입니다. 다음은 4 가지 주요 클래스로 구성된 분류법입니다 (Bach and Harnish, 1979).

> 언어행위는 "집단의 한 구성원이 방해받지 않고 생산한 하나의 발언이 특정 기능(또는 행위)로 다른 구성원들에게 받아들여지는, 집단 상호작용 과정"으로 정의할 수 있다.

```
Constatives: committing the speaker to something’s being the case (answering, claiming, confirming, denying, disagreeing, stating)
Directives: attempts by the speaker to get the addressee to do something (advising, asking, forbidding, inviting, ordering, requesting)
Commissives: committing the speaker to some future course of action (promising, planning, vowing, betting, opposing)
Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some social action (apologizing, greeting, thanking, accepting an acknowledgment)
```

```
답변 : 화자에게 무언가를 하기 (답변, 주장, 확인, 거부, 동의하지 않음, 진술)
지시 : 화자에 의해 수취인이 무언가를하도록 시도 (조언, 요청, 금지, 초대, 주문, 요청)
커미티브 : 화자에게 미래의 행동 과정 (약속, 계획, 투표, 베팅, 반대)
감사 : 사회적 행동 관련하여 청자에 대한 화자의 태도를 표현 (사죄, 인사, 감사, 인정)
```

A user asking a person or a dialogue system to do something (‘Turn up the music’) is issuing a DIRECTIVE. Asking a question that requires an answer is also a way of issuing a DIRECTIVE: in a sense when the system says (C2) "what day in May did you want to travel?" it’s as if the system is (very politely) commanding the system to answer. By contrast, a user stating a constraint (like C1 ‘I need to travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing an ACKNOWLEDGMENT. The speech act expresses an important component of the intention of the speaker (or writer) in saying what they said.

 이 사용자는 개인이나 대화 시스템에게 무언가를 요청하는('음악 켜기') DIRECTIVE를 시작하고 있습니다. 답변이 필요한 질문을 하는 것도 DIRECTIVE를 시작하는 방법입니다. 시스템이 (A1)"5월 어느 날 여행하고 싶습니까?" 라고 말하는 것을 보면 마치 (매우 정중하게) 응답하도록 명령하는 것처럼 보입니다. 이와 대조적으로 (C1)"5월에 여행해야 해요"와 같은 제약 조건을 명시한 사용자는 CONSTATIVE를 시작하고 있습니다. 시스템에 감사하는 사용자가 ACKNOWLEDGMENT를 시작하고 있습니다. 발화 행위는 화자(혹은 글쓴이)가 말할 때 말하려는 의도의 중요 구성 요소를 나타냅니다.

> system says (C2) "what day in May did you want to travel?" 는 오타가 아닌가 싶음.

### Grounding (확인 작업)

A dialogue is not just a series of independent speech acts, but rather a collective act performed by the speaker and the hearer. Like all collective acts, it’s important for the participants to establish what they both agree on, called the common ground(Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Grounding means acknowledging that the hearer has understood the speaker; like an ACK used to confirm receipt in data communications (Clark, 1996). (People need grounding for non-linguistic actions as well; the reason an elevator button lights up when it’s pressed is to acknowledge that the elevator has indeed been called (Norman, 1988).)

대화는 일련의 독립적인 발화 행위가 아니라 발화자와 청취자가 수행하는 집단적 행위입니다. 모든 집단 행위와 마찬가지로 참가자들이 상호 확인이라고하는 양쪽 모두가 동의했다는 것을 확인하는 것이 중요합니다 (Stalnaker, 1978). 화자들은 서로의 말을 근거로 이 것을 수행합니다. 확인의 의미는 청자가 화자를 이해했음을 인정하는 것입니다. 데이터 통신 수신을 확인하는 데 사용되는 ACK처럼 (Clark, 1996). (사람들은 비언어적 행동을 위해서도 접지가 필요하다; 엘리베이터 버튼을 눌렀을 때 엘리베이터 버튼이 점등되는 이유는 엘리베이터가 실제로 호출되었다는 것을 확인하기 위해서이다 (Norman, 1988).)

Humans constantly ground each other’s utterances. We can ground by explicitly saying "OK", as the agent does in A8 or A10. Or we can ground by repeating what the other person says; in utterance A1 the agent repeats "in May", demonstrating her understanding to the client. Or notice that when the client answers a question, the agent begins the next question with "And". The "And" implies that the new question is ‘in addition’ to the old question, again indicating to the client that the agent has successfully understood the answer to the last question.

인간은 끊임없이 서로의 발화를 확인합니다. 상담원이 A8 또는 A10에서 하는 것처럼 "OK"라고 명시적으로 말하면 됩니다. 아니면 발언 A1에서 상담원은 "5월에" 반복하여 고객에 대한 이해를 보여준것처럼 상대방이 말한 것을 반복함으로써 확인할 수 있습니다. 혹은 고객이 질문에 대답하면 상담원은 "그리고"로 다음 질문을 시작합니다. "그리고"는 새로운 질문이 이전 질문에 '추가 된 것'임을 암시하며 상담원이 마지막 질문에 대한 답변을 성공적으로 이해했음을 다시 고객에게 확인 시킵니다.


### Subdialogues and Dialogue Structure

Conversations have structure. Consider, for example, the local structure between speech acts discussed in the field of conversational analysis (Sacks et al., 1974). QUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed by ACCEPTANCE (or REJECTION). COMPLIMENTS ("Nice jacket!") often give rise to adjacency pair DOWNPLAYERS ("Oh, this old thing?"). These pairs, called adjacency pairs are composed of a first pair part and a second pair part (Schegloff, 1968), and these expectations can help systems decide what actions to take.

대화는 구조를 갖고 있습니다. 예를 들어, 대화 분석 분야에서 논의 된 언어 행위 사이의 지역적 구조를 고려하십시오 (Sacks et al., 1974). "질문"에는 "답변"이 올 것이라 기대합니다. "제안"에는 "수락"(또는 "거부")이 따라 옵니다. "칭찬"("Nice jacket!")은 종종 "무시"("Oh, this old thing?")가 쌍으로 생기기도 합니다. 인접 쌍이라고하는 이러한 쌍은 첫 번째 쌍 파트와 두 번째 쌍 파트로 구성되며 (Schegloff, 1968) 이러한 예상은 시스템이 수행 할 작업을 결정하는 데 도움이 될 수 있습니다.

However, dialogue acts aren’t always followed immediately by their second pairside sequence part. The two parts can be separated by a side sequence (Jefferson 1972) or subdialogue. For example utterances C9 to A10 constitute a correction subdialogue(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):

그러나 대화 행위가 항상 두 번째 순서에 바로 이어서 오는 것은 아닙니다. 두 부분은 사이드 시퀀스 (Jefferson 1972) 또는 하위 대화로 분리 할 수 있습니다. 예를 들어, 발화 C9에서 A10은 수정 부분대화(correction subdialogue) (Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998)를 구성합니다.

```
C9: #Act. . . actually#, what day of the week is the 15th?
A9: It’s a Friday.
C10: Uh hmm. I would consider staying there an extra day til Sunday.
A10: OK. . . OK. On Sunday I have . . .
```

The question in C9 interrupts the prior discourse, in which the agent was looking for a May 15 return flight. The agent must answer the question and also realize that‘’I would consider staying...til Sunday" means that the client would probably like tochange their plan, and now go back to finding return flights, but for the 17th. Another side sequence is the clarification question, which can form a subdialogue between a REQUEST and a RESPONSE. This is especially common in dialogue systems where speech recognition errors causes the system to have to ask for clarifications or repetitions like the following:

C9의 질문은 상담원이 5 월 15 일 돌아오는 비행기를 찾고 있던 이전 담화를 중단시킵니다. 상담원은 질문에 대답하고 '일요일까지 머무를 것을 고려한다'라는 말은 고객이 계획을 변경하고 다시 귀국 항공편을 찾는 중이지만 17 일에는 다시 돌아온다는 것을 의미합니다. 다른 세부 대화 진행은 확인을 위한 질문이며, 요청과 응답 사이에 하위 대화를 형성 할 수 있습니다. 이것은 음성 인식 오류로 인해 시스템이 다음과 같이 설명이나 반복을 요구해야하는 대화 시스템에서 특히 일반적입니다.

```
User: What do you have going to UNKNOWN_WORD on the 5th?
System: Let’s see, going where on the 5th?
User: Going to Hong Kong.
System: OK, here are some flights...
```

In addition to side-sequences, questions often have presequences, like the following example where a user starts with a question about the system’s capabilities("Can you make train reservations") before making a request.

부가적인 순서 외에도, 질문은 종종 사용자가 요청하기 전에 시스템 기능("기차 예약 가능")에 대한 질문으로 시작하는 다음 예와 같은 사전 순서를 갖습니다.

```
User: Can you make train reservations?
System: Yes I can.
User: Great, I’d like to reserve a seat on the 4pm train to New York
```

### Initiative(주도권)

Sometimes a conversation is completely controlled by one participant. For example a reporter interviewing a chef might ask questions, and the chef responds. We say initiative that the reporter in this case has the conversational initiative (Walker and Whittaker,1990). In normal human-human dialogue, however, it’s more common for initiativeto shift back and forth between the participants, as they sometimes answer questions,sometimes ask them, sometimes take the conversations in new directions, sometimes not. You may ask me a question, and then I respond asking you to clarify something you said, which leads the conversation in all sorts of ways. We call such interactions mixed initiative.

대화 중 한 참가자가 완전히 주도권을 갖는 종류도 있습니다. 예를 들어 요리사를 인터뷰 경우 기자가 질문을하고 요리사가 응답합니다. 우리는 이 경우에 기자가 대화 주도권을 갖고 있다고 말합니다(Walker and Whittaker, 1990). 그러나 일반적인 인간과 인간의 대화에서는 참가자들이 때때로 질문에 대답하고 때로는 질문하거나 때로는 새로운 방향으로 대화를 나누는 경우가 종종 있습니다. 당신은 저에게 질문을 할 수 있습니다. 그리고 나는 당신이 말한 것을 분명히하도록 요청하며, 그것은 모든 종류의 방식으로 대화를 이끌어냅니다. 우리는 그러한 상호 작용을 혼합 주도권(mixed initiative)이라고 부릅니다.

Mixed initiative, while the norm for human-human conversations, is very difficult for dialogue systems to achieve. It’s much easier to design dialogue systems to be passive responders. In the question answering systems we saw in Chapter 25, or in simple search engines, the initiative lies completely with the user. In such user initiative systems, the user specifies a query, and the systems responds. Then the user can specify another query. Alternatively, you may have had the experience of being stuck in a bad dialogue system that asks a question and gives you no opportunity to do anything until you answer it. Such system-initiative architectures can bevery frustrating.

혼합 주도권은 인간과 인간의 대화에 에서 일반적으로 보여지지만 대화 시스템이 달성하기가 매우 어렵습니다. 대화 시스템을 수동적으로 응답하도록 하는것이 훨씬 쉽게 설계 할 수 있습니다. 25 장이나 간단한 검색 엔진에서 보았던 질문 응답 시스템에서 주도권은 사용자에게 전적으로 달려 있습니다. 이러한 사용자 주도권 시스템에서 사용자는 쿼리를 지정하고 시스템이 응답합니다. 그런 다음 사용자는 다른 쿼리를 지정할 수 있습니다. 혹은 시스템이 질문한 다음 사용자가 대답하기 전까지 아무것도 할 수 없는 나쁜 대화 시스템에 갇힌 경험이 있었을 수도 있습니다. 이러한 시스템 주도적 아키텍처는 매우 실망 스러울 수 있습니다.


### Inference and Implicature(추론과 함의)

Inference is also important in dialogue understanding. Consider the client’s response C2, repeated here:

추론은 대화를 이해하는데 중요한 요소입니다. 사용자의 답변 중 C2를 살펴봅시다.

```
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
```

Notice that the client does not in fact answer the agent’s question. The client merely mentions a meeting at a certain time. What is it that licenses the agent to infer that the client is mentioning this meeting so as to inform the agent of the travel dates?

고객은 실제로 상담원의 질문에 답변하지 않습니다. 클라이언트는 단지 특정 시간에 회의를 언급할 뿐입니다. 상담원에게 여행 날짜를 알리기 위해 고객이이 미팅을 언급하고 있다고 추론 할 수있는 것은 무엇입니까?

The speaker seems to expect the hearer to draw certain inferences; in other words, the speaker is communicating more information than seems to be present in the uttered words. This kind of example was pointed out by Grice (1975, 1978) as part of his theory of conversational implicature. Implicature means a particular class of licensed inferences. Grice proposed that what enables hearers to draw these inferences is that conversation is guided by a set of maxims, general heuristics that play a guiding role in the interpretation of conversational utterances. 

화자는 청자가 특정한 추론을 이끌어 내기를 기대하는 것으로 보입니다. 다시 말해, 화자는 말로 표현 된 것보다 더 많은 정보를 전달하고 있는 것입니다. 이런 종류의 예는 그의 대화적 함의 이론의 일부로서 Grice (1975, 1978)가 언급하였습니다. 함의(Implicature)는 일부 인정받은 추론들을 의미합니다. Grice는 대화 중 이러한 추론을 도출해 내는데 있어서 발화의 해석에 주도적인 역할을 하는 것은 일련의 격률(maxim, 상식?), 일반적인 경험에 의해 유도된다는 것이라고 제안하였습니다. 

> https://ko.wikipedia.org/wiki/그라이스의_대화격률

One such maxim is the maxim of relevance which says that speakers attempt to be relevant, they don’t just utter random speech acts. When the client mentions a meeting on the 12th, the agent reasons ‘There must be some relevance for mentioning this meeting. What could it be?’. The agent knows that one precondition for having a meeting(at least before Web conferencing) is being at the place where the meeting is held, and therefore that maybe the meeting is a reason for the travel, and if so, then since people like to arrive the day before a meeting, the agent should infer that the flight should be on the 11th.

이러한 격률 중 하나는 화자가 말을할 때 관련 있는 것들을 말하지 관련성 없는 말을 무작위로 내뱉지 않는다는 관련성의 격률(the maxim of relevance) 입니다. 고객이 12일에 회의에 대해 언급 할 때 상담원은 "이 회의를 언급하는 데는 어떤 관련성이 있을 것이다. 무슨 일이지?" 라고 추론할 수 있습니다. 상담원은 회의를 개최하기위한 장소 (최소한 웹 회의 전)에 대한 하나의 전제 조건이 진행 중이므로 회의가 여행의 이유 일 수 있다는 것을 알고 있습니다. 회의 전날에 도착하는 경우, 상담원은 비행이 11일에 있어야한다고 추론할 수 있습니다.

These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research.

인간 대화의 이러한 미묘한 특성 (턴, 스피치 행위, 확인 작업, 대화 구조, 주도 및 함축)은 인간과 자연스럽게 대화 할 수있는 대화 시스템을 구축하기 어려운 이유 중 하나입니다. 이러한 과제 중 다수는 대화 시스템 연구의 활발한 분야입니다.

## Chatbots

The simplest kinds of dialogue systems are chatbots, systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or ‘chats’ characteristic of informal human-human interaction.

가장 간단한 종류의 대화 시스템은 구조화되지 않은 대화를 모방하거나 비공식적 인 인간-인간 상호 작용의 '채팅' 특성을 목표로 확장 된 대화를 수행할 수있는 챗봇입니다.

These systems often have an entertainment value, such as Cleverbot, a simple IR based chatbot capable of carrying on the kinds of conversations shown in Fig. 26.2,or Microsoft’s XiaoIce system(Zhou et al., 2018), a more sophisticated commercial system that chats with people on text messaging platforms (Fig. 26.3). Yet starting from the very first system, ELIZA (Weizenbaum, 1966), chatbots have also been used for practical purposes, such as testing theories of psychological counseling.

그림 26.2에 표시된 종류의 대화를 수행 할 수 있는 엔터테인먼트 분야의 간단한 IR 기반 챗봇 인 Cleverbot나 보다 정교한 상용 시스템 인 Microsoft의 XiaoIce 시스템(Zhou et al., 2018)이 있으며 이들은 문자 메시지 플랫폼에서 사용자와 채팅합니다 (그림 26.3). 그러나 최초의 시스템 인 ELIZA (Weizenbaum, 1966)부터 챗봇은 심리 상담 이론 테스트와 같은 실용적인 목적으로도 사용되었습니다.

Like practically everything else in language processing, chatbot architectures fall into two classes: rule-based systems and corpus-based systems. Rule-based systems include the early influential ELIZA and PARRY systems. Corpus-based systems mine large datasets of human-human conversations, which can be done by using information retrieval (IR-based systems simply copy a human’s response from a previous conversation) or by using a machine translation paradigm such as neural network sequence-to-sequence systems, to learn to map from a user utterance to a system response.

언어 처리의 다른 모든 것과 마찬가지로 챗봇 아키텍처는 규칙 기반 시스템과 말뭉치 기반 시스템의 두 가지 클래스로 분류됩니다. 규칙 기반 시스템에는 초기 영향력있는 ELIZA 및 PARRY 시스템이 포함됩니다. 말뭉치 기반 시스템은 정보 검색 (IR 기반 시스템은 단순히 이전 대화에서 인간의 응답을 복사 함)을 사용하거나 신경망 시퀀스와 같은 기계 번역 패러다임을 사용하여 인간-인간 대화의 대규모 데이터 세트를 발굴해 냅니다. 그리고 이것을 사용자 발화에서 시스템 응답으로 매핑하는 방법을 배웁니다.

### 26.2.1 Rule-based chatbots: ELIZA and PARRY

ELIZA (Weizenbaum, 1966) is the most important chatbot dialogue system in the history of the field, and so we take some time here to describe its algorithm in detail. ELIZA was designed to simulate a Rogerian psychologist, based on a branchof clinical psychology whose methods involve drawing the patient out by reflectingpatient’s statements back at them. Rogerian psychology is the rare type of conversation in which, as Weizenbaum points out, one can "assume the pose of knowingal most nothing of the real world". If a patient says "I went for a long boat ride" andthe psychiatrist says "Tell me about boats", you don’t assume she didn’t know whata boat is, but rather assume she had some conversational goal. Most chatbots tryingto pass the Turing test choose a domain with similar properties.

ELIZA (Weizenbaum, 1966)는 이 분야의 역사상 가장 중요한 챗봇 대화 시스템이므로 알고리즘에 대해 자세히 설명하는 데 시간이 걸립니다. 엘리자(ELIZA)는 환자의 진술을 다시 반영하여 환자를 끌어내는 방법을 포함하는 임상 심리학의 한 가지를 기반으로 Rogerian 심리학자를 시뮬레이션하도록 설계되었습니다. Rogerian 심리학은 Weizenbaum이 지적한 것 처럼 "현실 세계를 거의 아는 것이 없다고 가정"하는 드문 유형의 대화입니다. 환자가 "배를 길게 타러 갔다"고 정신과 의사가 "배에 대해 말해줘"라고 말하면 보트가 무엇인지 몰랐다고 가정하는 것이 아니라 대화 목표가 있다고 가정합니다. Turing 테스트를 통과하려는 대부분의 챗봇은 비슷한 속성을 가진 도메인을 선택합니다.

```
Men are all alike.
IN WHAT WAY
They’re always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
..
..
WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?
My father
YOUR FATHER
You are like my father in some ways.
WHAT RESEMBLANCE DO YOU SEE
You are not very aggressive but I think you don’t want me to notice that.
WHAT MAKES YOU THINK I AM NOT AGGRESSIVE
You don’t argue with me.
WHY DO YOU THINK I DON’T ARGUE WITH YOU
You are afraid of me.
DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU
My father is afraid of everybody.
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER
Bullies.
DOES THAT HAVE ANYTHING TO DO W
```

fig 26.4

Fig. 26.4 shows the most famous ELIZA conversation, in which ELIZA seems to simulate giving very sophisticated therapeutic advice, excerpted (deleting some material with ...) from a full interaction given in the original paper (Weizenbaum, 1966). 

As we summarized briefly in Chapter 2, ELIZA worked by pattern/transform rules like the following one:

그림 26.4는 가장 유명한 ELIZA 대화를 보여줍니다. ELIZA는 매우 정교한 치료 적 조언을 제공하는 것처럼 보이고, 논문에서 주어진 완전한 상호 작용에서 발췌 한 (일부 자료 삭제) ... (Weizenbaum, 1966).

2 장에서 간략하게 요약 한 것처럼 ELIZA는 다음과 같은 패턴 / 변형 규칙에 따라 작업했습니다.

```
(0 YOU 0 ME) [pattern]
->
(WHAT MAKES YOU THINK I 3 YOU) [transform]
```

In the ELIZA pattern, 0 means Kleene* , and in the transform rules the numbers are the index of the constituent in the pattern. Thus the number 3 refers to the second 0 in the first pattern. This rule would transfer

ELIZA 패턴에서 0은 Kleene *을 의미하고 변환 규칙에서 숫자는 패턴의 구성 요소의 인덱스입니다. 따라서 숫자 3은 첫 번째 패턴에서 두 번째 0을 나타냅니다. 이 규칙은

```
You hate me
into:
WHAT MAKES YOU THINK I HATE YOU
```

Each ELIZA pattern/rule is linked to a keyword that might occur in a user sentence. The architecture is summarized in Fig. 26.5.

Keywords are associated with a rank, with specific words being more highly ranked, and more general words ranking lower. Consider the following user sentence:

각 ELIZA 패턴 / 규칙은 사용자 문장에서 발생할 수있는 키워드에 연결됩니다. 아키텍처는 그림 26.5에 요약되어 있습니다.

키워드는 순위와 관련이 있으며 특정 단어의 순위가 높고 일반적인 단어의 순위가 낮습니다. 다음 사용자 문장을 고려하십시오.

```
function ELIZA GENERATOR(user sentence) returns response

Find the word w in sentence that has the highest keyword rank
    if w exists
        Choose the highest ranked rule r for w that matches sentence
        response←Apply the transform in r to sentence
        if w = ‘my’
            future←Apply a transformation from the ‘memory’ rule list to sentence
            Push future onto memory stack
        else (no keyword applies)
            either
                response←Apply the transform for the NONE keyword to sentence
            or
                response←Pop the top response from the memory stack
    return(response)
```

Figure 26.5 A simplified sketch of the ELIZA algorithm. The power of the algorithm comes from the particular transforms associated with each keyword.

```
I know everybody laughed at me
```

Because it has the word "I", this sentence could match the following rule whose keyword is I:

"I"라는 단어가 있기 때문에이 문장은 키워드가 I 인 다음 규칙과 일치 할 수 있습니다.

```
(I *) -> (You say you 2)
```

producing:

```
YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU
```

But as Weizenbaum points out, "I" is a very general word and its keywords lead to very general responses. Instead the keyword "everybody" is much more interesting, since someone using universals like everybody or always is probably "referring to some quite specific event or person". 
Therefore, ELIZA prefers to respond with the pattern associated with the more specific keyword everybody (implementing by just assigning "everybody" rank 5 and "I" rank 0 in the lexicon), whose rule thus transforms the sentence to: 


그러나 Weizenbaum이 지적했듯이 "I"는 매우 일반적인 단어이며 키워드는 매우 일반적인 반응을 낳습니다. 대신 "모두"라는 키워드는 훨씬 더 흥미 롭습니다. 왜냐하면 모든 사람과 같은 보편적인 것을 사용하거나 항상 "특정한 사건이나 사람을 언급하는" 사람이기 때문입니다.
따라서 ELIZA는 모든 사람에게보다 구체적인 키워드 (어휘집에서 "모든 사람"순위 5 및 "I"순위 0을 할당하여 구현)와 관련된 패턴으로 응답하는 것을 선호합니다.이 규칙은 문장을 다음과 같이 변환합니다.

```
WHO IN PARTICULAR ARE YOU THINKING OF?
```

If no keyword matches, ELIZA chooses a non-commital response like "PLEASE GO ON", "THAT’S VERY INTERESTING", or "I SEE". Finally, ELIZA has a clever memory trick that accounts for the very last sentence of the conversation above. 
Whenever the word "my" is the highest ranked keyword, ELIZA will randomly select a transform on the MEMORY list, apply it to the sentence, and store it on the stack

일치하는 키워드가없는 경우 ELIZA는 "PLEASE GO ON", "매우 흥미 롭습니다"또는 "나는 본다"와 같은 커밋되지 않은 응답을 선택합니다. 마지막으로, ELIZA는 위 대화의 마지막 문장을 설명하는 영리한 메모리 트릭을 가지고 있습니다.
"my"라는 단어가 가장 높은 순위의 키워드 일 때마다 ELIZA는 MEMORY 목록에서 임의로 변환을 선택하여 문장에 적용하고 스택에 저장합니다.

```
(MEMORY MY
    (0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)
    (0 MY 0 = EARLIER YOU SAID YOUR 3)
    (0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3
```

Later, if no keyword matches a sentence, ELIZA will return the top of the MEMORY queue instead.

People became deeply emotionally involved with the program. Weizenbaum tells the story of one of his staff who would ask Weizenbaum to leave the room when she talked with ELIZA. When Weizenbaum suggested that he might want to store all the ELIZA conversations for later analysis, people immediately pointed out the privacy implications, which suggested that they were having quite private conversations with ELIZA, despite knowing that it was just software. ELIZA’s framework is still used today; modern chatbot system tools like ALICE are based on updated versions of ELIZA’s pattern/action architecture. A few years after ELIZA, another chatbot with a clinical psychology focus, PARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZAlike regular expressions, the PARRY system included a model of its own mental state, with affect variables for the agent’s levels of fear and anger; certain topics of conversation might lead PARRY to become more angry or mistrustful. If PARRY’s anger variable is high, he will choose from a set of "hostile" outputs. If the input mentions his delusion topic, he will increase the value of his fear variable and then begin to express the sequence of statements related to his delusion. Parry was the first known system to pass the Turing test (in 1972!); psychiatrists couldn’t distinguish text transcripts of interviews with PARRY from transcripts of interviews with real paranoids (Colby et al., 1972).

나중에 문장과 일치하는 키워드가 없으면 ELIZA는 대신 MEMORY 대기열의 맨 위를 반환합니다.

사람들은 프로그램에 깊이 감정적으로 참여했습니다. Weizenbaum은 직원 중 한 명이 Weizenbaum에게 ELIZA와 대화 할 때 방을 나가라고 요청합니다. Weizenbaum이 나중에 분석하기 위해 모든 ELIZA 대화를 저장하고 싶을 때 사람들은 즉시 개인 정보 보호 관련 사항을 지적했으며, 이는 소프트웨어 일 뿐이라는 사실에도 불구하고 ELIZA와 사적인 대화를 하고 있음을 시사했습니다. ELIZA의 프레임워크는 오늘날에도 여전히 사용되고 있습니다. ALICE와 같은 최신 챗봇 시스템 도구는 업데이트 된 ELIZA의 패턴 / 액션 아키텍처를 기반으로합니다. ELIZA 후 몇 년 후 임상 심리학에 중점을 둔 또 다른 챗봇 인 PARRY (Colby et al., 1971)가 정신 분열증을 연구하는 데 사용되었습니다. PARRY 시스템은 ELIZA와 같은 정규 표현 외에도 에이전트의 두려움과 분노 수준에 영향을 미치는 자체 정신 상태 모델을 포함했습니다. 대화의 특정 주제는 PARRY가 더 화 나거나 불신하게 될 수 있습니다. PARRY의 분노 변수가 높으면 일련의 "적대적"출력에서 선택합니다. 입력이 그의 망상 주제를 언급하면, 그는 그의 공포 변수의 가치를 증가시키고 그의 망상과 관련된 진술의 순서를 표현하기 시작할 것입니다. Parry는 1972 년 Turing 테스트를 통과 한 최초의 시스템이었습니다. 정신과 전문의는 PARRY와의 인터뷰 내용을 실제 편집증과의 인터뷰 내용과 구별 할 수 없었습니다 (Colby et al., 1972).

### 26.2.2 Corpus-based chatbots

Corpus-based chatbots, instead of using hand-built rules, mine conversations of human-human conversations, (or sometimes mine the human sides of human-machine conversations).

These systems are enormously data-intensive; Serban et al. (2018) estimate that training modern chatbots require hundreds of millions or even billions of words. Many such corpora have been used, including large spoken conversational corpora like the Switchboard corpus of American English telephone conversations (Godfrey et al., 1992) or the various CALLHOME and CALLFRIEND telephone conversational corpora in many languages. Many systems also train on movie dialogue, which is available in great quantities in various corpora (Lison and Tiedemann, 2016, inter alia), and which resembles natural conversation in many ways (Forchini, 2013). Text from microblogging sites like Twitter (Ritter et al., 2010) or a Weibo (微博) have also been used, or datasets of crowd worker conversations like Topical-Chat (Gopalakrishnan et al., 2019). Many corpora also focus on specific topics, and can be used for topical chatbots. See Serban et al. (2018) for a comprehensive summary of available corpora. Another common technique is to extract possible responses from non-dialogue corpora, so that a chatbot can tell stories or mention facts acquired in that way. 

말뭉치 기반 챗봇은 직접 만든 규칙을 사용하는 대신 인간과 인간 사이의 대화를 채굴하거나 때로는 인간과 기계 사이의 대화를 채굴합니다. 

이 시스템은 데이터 집약적입니다. Serban et al. (2018)에 따르면 현대 챗봇을 훈련하려면 수억 또는 수십억 단어가 필요하다고 추정했습니다. 미국 영어 전화 대화의 Switchboard 말뭉치 (Godfrey et al., 1992)와 같은 대규모 대화 말뭉치들 또는 여러 언어로 된 다양한 CALLHOME 및 CALLFRIEND 상황에서 전화 대화 말뭉치와 같은 많은 말뭉치들이 사용되었습니다. 많은 시스템들이 영화 대화로 훈련 시키며,이 영화는 다양한 말뭉치들에서 대량으로 이용 가능하며 (Lison and Tiedemann, 2016, 특히), 여러 가지 면에서 자연스런 대화와 유사합니다 (Forchini, 2013). Twitter (Ritter et al., 2010) 또는 Weibo (微 博)와 같은 마이크로 블로그 사이트의 텍스트 또는 Topical-Chat (Gopalakrishnan et al., 2019)과 같은 대규모 업무 대화 데이터 세트가 사용되었습니다. 많은 말뭉치들 또한 특정 주제에 중점을 두고 있으며 주제별 챗봇에 사용될 수 있습니다. Serban et al. (2018)에서 사용 가능한 말뭉치들의 종합적인 요약을 볼 수 있습니다. 또 다른 일반적인 기술은 대화 상대가 아닌 말뭉치들에서 가능한 응답을 추출하여 챗봇이 이야기를하거나 그러한 방식으로 얻은 사실을 언급 할 수 있도록 하는 것입니다. 

Finally, once a chatbot has been put into practice, the turns that humans use to respond to the chatbot can be used as additional conversational data for training. The XiaoIce system collects and stores all human-machine conversations between XiaoIce and its users, resulting in a a dataset of over 30 billion conversation pairs. It’s crucial in these cases to remove personally identifiable information (PII); see Section 26.6.1. The two main architectures for corpus-based chatbots: information retrieval, and machine learned sequence transduction. Like rule-based chatbots (but unlike framebased dialogue systems), most corpus-based chatbots do very little modeling of the conversational context. Instead they tend to focus on generating a single response turn that is appropriate given the user’s immediately previous utterance or two. For this reason they are often called response generation systems. Corpus-based chat- response generation bots thus have some similarity to question answering systems, which focus on single responses while ignoring context or larger conversational goals.

마지막으로, 챗봇이 실행되면 사람들이 챗봇에 응답하기 위해 사용한 턴은 훈련을 위한 추가 대화 데이터로 사용될 수 있습니다. XiaoIce 시스템은 XiaoIce와 사용자 간의 모든 인간-기계 대화를 수집하고 저장하므로 300 억 개 이상의 대화 쌍의 데이터 세트가 생성됩니다. 이러한 경우 개인 식별 정보 (PII)를 제거하는 것이 중요합니다. 섹션 26.6.1을 참조하십시오. 말뭉치 기반 챗봇의 두 가지 주요 아키텍처 : 정보 검색 및 머신 러닝 시퀀스 변환 입니다. 규칙 기반 챗봇 (프레임 기반 대화 시스템과는 달리)과 마찬가지로 대부분의 말뭉치 기반 챗봇은 대화 컨텍스트를 거의 모델링하지 않습니다. 대신에, 그들은 사용자의 직전 발언이 적절한 경우 단일 응답 턴을 생성하는 데 집중하는 경향이 있습니다. 이러한 이유로 종종 응답 생성 시스템이라고합니다. 말뭉치 기반 채팅 응답 생성 봇은 질문 응답 시스템과 일부 유사하므로 문맥 또는 더 큰 대화 목표를 무시하고 단일 응답에 중점을 둡니다.

#### IR-based chatbots

The principle behind information retrieval based chatbots is to respond to a user’s turn X by repeating some appropriate turn Y from a corpus of natural (human) text of the sort described in the prior section.  

정보 검색 기반 챗봇의 기본 원리는 이전 섹션에서 설명한 종류의 자연 (인간) 텍스트 모음에서 적절한 턴Y를 반복하여 사용자의 턴X에 응답하는 것입니다.

Given the corpus and the user’s sentence, IR-based systems can use any retrieval algorithm to choose an appropriate response from the corpus. The two simplest methods are the following: 

말뭉치와 사용자의 문장이 주어지면 IR 기반 시스템은 검색 알고리즘을 사용하여 말뭉치에서 적절한 응답을 선택할 수 있습니다. 가장 간단한 두 가지 방법은 다음과 같습니다. 

1. Return the response to the most similar turn: Given user query q and a conversational corpus C, find the turn t in C that is most similar to q (for example has the highest cosine with q) and return the following turn, i.e. the human response to t in C:

1. 가장 유사한 턴에 대한 응답을 반환합니다. 사용자 쿼리 q와 대화식 말뭉치 C가 주어지면 q와 가장 유사한 C에서 턴 t를 찾습니다 (예 : q와 코사인이 가장 높은 경우) ) 다음 차례, 즉 C의 t에 대한 사람의 응답을 반환합니다.

```
r = response(argmax_t∈C * q^T * t / ||q| * |t||)
```

The idea is that we should look for a turn that most resembles the user’s turn, and return the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011).

아이디어는 사용자의 턴과 가장 유사한 턴을 찾아 그 턴의 사람의 응답을 돌려주는 것입니다 (Jafarpour et al. 2009, Leuski and Traum 2011).

**2. Return the most similar turn**: Given user query q and a conversational corpus C, return the turn t in C that is most similar to q (for example has the highest cosine with q):

** 2. 가장 유사한 턴 반환 ** : 사용자 쿼리 q와 대화식 말뭉치 C가 주어지면 q와 가장 유사한 C의 턴t를 반환합니다 (예 : q와 코사인이 가장 높은 경우).

```
r = argmax_t∈C * q^T * t / ||q| * |t||
```

The idea here is to directly match the users query q with turns from C, since a good response will often share words or semantics with the prior turn. 

이 아이디어는 사용자 쿼리 q를 C의 턴과 직접 일치시키는 것이기 때문에 보통 같은 단어를 쓰거나 이전 턴에 같은 의미를 지니고 있을 때 좋은 응답을 얻을 수 있습니다. (Ritter et al. 2011). , Wang 등 2013). 

In each case, any similarity function can be used, such as cosines computed either over words (weighted by tf-idf) or more commonly now, cosines over any kind of sentence embeddings. 


각각의 경우에, 단어를 통해 계산 된 코사인 (tf-idf에 의해 가중 된) 또는 보다 일반적으로 모든 종류의 문장 임베딩에 대한 코사인과 같은 유사성 함수가 사용될 수 있습니다. 

Although returning the response to the most similar turn seems like a more intuitive algorithm, returning the most similar turn seems to work better in practice, perhaps because selecting the response adds another layer of indirection that can allow for more noise. (Ritter et al. 2011, Wang et al. 2013).  

가장 유사한 턴의 응답을 반환하는 것이 더 직관적인 알고리즘처럼 보이지만, 가장 유사한 턴을 반환하는 것은 실제로 더 잘 작동합니다. 이것은 아마도 응답을 선택할 때 또다른 간접 레이어가 추가될 수 있고 이것은 또 다른 노이즈가 낄 수 있기 때문입니다.

The IR-based approach can be extended by using more features than just the words in the q. For example using the entire conversation with the user so far can be quite helpful when the user’s query is short (like "Yes" or "OK"). Information about the user or sentiment or other information can also play a role. The IR-based approach can even draw responses from narrative (non-dialogue) text. The COBOT chatbot (Isbell et al., 2000) pioneered this approach, generating responses by selecting sentences from a corpus that combined the Unabomber Manifesto by Theodore Kaczynski, articles on alien abduction, the scripts of "The Big Lebowski" and "Planet of the Apes". Chatbots that want to generate informative turns such as answers to user questions can use texts like Wikipedia to draw on sentences that might contain those answers (Yan et al., 2016). XiaoIce similarly collects sentences from public lectures and news articles and searches them using IR based on query expansion from the user’s turn to respond to turns like "Tell me something about Beijing" (Zhou et al., 2018).

q에있는 단어보다 더 많은 기능을 사용하여 IR 기반 접근 방식을 확장 할 수 있습니다. 예를 들어, 지금까지 사용자와의 전체 대화를 사용하면 사용자의 쿼리가 짧은 경우 (예 : "예"또는 "확인") 매우 유용 할 수 있습니다. 사용자나 정서 또는 기타 사항에 대한 정보도 중요한 역할을 합니다. IR 기반 접근 방식은 대화식 (비 대화식) 텍스트에서 응답을 이끌어 낼 수도 있습니다. COBOT 챗봇(Isbell et al., 2000)이 처음으로 시도했는데 Theodore Kaczynski의 Unabomber Manifesto와 외계인 납치 관련 기사,"The Big Lebowski" 및 "Planet of the Plana 원숭이"에서 응답을 생성했습니다. 사용자 질문에 대한 답변과 같은 유익한 전환을 생성하려는 챗봇은 Wikipedia와 같은 텍스트를 사용하여 해당 답변이 포함 된 문장을 그릴 수 있습니다 (Yan et al., 2016). XiaoIce는 유사하게 공개 강의 및 뉴스 기사에서 문장을 수집하고 사용자의 턴에 따른 쿼리 확장을 기반으로한 IR을 사용하여 "베이징에 대해 뭔가 말해줘"와 같은 턴에 응답하여 문장을 검색합니다 (Zhou et al., 2018).

#### Encoder decoder chatbots

```
"What ho!" I said.
"What ho!" said Motty.
"What ho! What ho!"
"What ho! What ho! What ho!"
After that it seemed rather difficult to go on with the conversation."
```

An alternate way to use a corpus to generate dialogue is to think of response generation as a task of transducing from the user’s prior turn to the system’s turn. This is basically the machine learning version of Eliza; the system learns from a corpus to transduce a question to an answer. This idea was first developed by using phrase-based machine translation (Ritter et al., 2011) to translate a user turn to a system response. It quickly became clear, however, that the task of response generation was too different from machine translation. In machine translation, words or phrases in the source and target sentences tend to align well with each other, but in conversation, a user utterance may share no words or phrases with a coherent response. Instead, (roughly contemporaneously by y Shang et al. 2015, Vinyals and Le 2015, and Sordoni et al. 2015) transduction models for response generation were modeled instead using encoder-decoder models (Chapter 11), as shown in Fig. 26.6. 

fig 26.6

말뭉치를 사용하여 대화를 생성하는 다른 방법은 응답 생성을 사용자의 이전 턴에서 시스템 턴로 전환하는 작업으로 생각하는 것입니다. 이것은 기본적으로 Eliza의 기계 학습 버전인데 시스템은 말뭉치로부터 학습하여 질문을 답변으로 변환합니다. 이 아이디어는 사용자턴에서 시스템 응답으로 변환하기 위한 구문 기반 기계 번역 (Ritter et al., 2011)을 사용하여 처음 개발되었습니다. 그러나 응답 생성 작업이 기계 번역과 너무 다르다는 것이 곧 분명해졌습니다. 기계 번역에서는 원본와 목표 문장의 단어나 구는 서로 잘 정렬되는 경향이 있지만 대화에서 사용자 발화는 일관된 응답으로 단어나 구를 공유하지 않을 수 있습니다. 대신에, 그림 26.6에 도시 된 바와 같이, 응답 생성을 위한(대략 동시 적으로 y Shang et al. 2015, Vinyals and Le 2015, Sordoni et al. 2015에 의해) 응답 생성을 위한 변환 모델인 인코더-디코더 모델 (제 11 장)을 사용하여 모델링 되었습니다.

그림 26.6

A number of modifications are required to the basic encoder-decoder model to adapt it for the task of response generation. For example basic encoder-decoder models have a tendency to produce predictable but repetitive and therefore dull responses like "I’m OK" or "I don’t know" that shut down the conversation.  This can be addressed by changing the objective function for seq2seq model training to a mutual information objective, or by modifying a beam decoder to keep more diverse responses in the beam (Li et al., 2016). Another problem with simple encoder-decoder response generation architecture is its inability to model the longer prior context of the conversation. This can be done by allowing the model to see prior turns, such as by using a hierarchical model that summarizes information over multiple prior turns (Lowe et al., 2017b). Finally, encoder-decoder response generators focus on generating single responses, and so don’t tend to do a good job of continuously generating responses that cohere across multiple turns. This can be addressed by using reinforcement learning, as well as techniques like adversarial networks, to learn to choose responses that make the overall conversation more natural (Li et al. 2016, Li et al. 2017). Fig. 26.7 shows some sample responses generated by a vanilla SEQ2SEQ model, and from a model trained by an adversarial algorithm to produce responses that are harder to distinguish from human responses (Li et al., 2017).

응답 생성 작업에 맞게 기본 인코더 디코더 모델을 수정하려면 많은 수정이 필요합니다. 예를 들어 기본 인코더 디코더 모델은 예측 가능하지만 반복적이며 대화를 종료시키는 "정말"또는 "모름"과 같은 둔감 한 응답을 생성하는 경향이 있습니다. 이는 seq2seq 모델 훈련을위한 목적 함수를 상호 정보 목표로 변경하거나 빔에서보다 다양한 응답을 유지하도록 빔 디코더를 수정하여 해결할 수 있습니다 (Li et al., 2016). 간단한 인코더-디코더 응답 생성 아키텍처의 또 다른 문제점은 대화의 더 긴 이전 컨텍스트를 모델링 할 수 없다는 것입니다. 예를 들어 여러 개의 이전 회전에 대한 정보를 요약하는 계층 적 모델을 사용하여 모델이 이전 회전을 볼 수 있도록하여이를 수행 할 수 있습니다 (Lowe et al., 2017b). 마지막으로, 인코더-디코더 응답 생성기는 단일 응답 생성에 중점을두기 때문에 여러 차례에 걸쳐 연속적으로 응답을 생성하는 데 적합하지 않습니다. 이 문제는 적대적 네트워크와 같은 기술뿐만 아니라 강화 학습을 사용하여 전반적인 대화를보다 자연스럽게 만드는 응답을 선택하는 방법을 배웁니다 (Li et al. 2016, Li et al. 2017). 그림 26.7은 바닐라 SEQ2SEQ 모델과 인간의 반응과 구별하기 어려운 반응을 생성하기 위해 적대 알고리즘에 의해 훈련 된 모델에서 생성 된 일부 샘플 응답을 보여줍니다 (Li et al., 2017).

```
Input 
tell me ... how long have you had this falling sickness ?
Vanilla-SEQ2SEQ 
i’m not a doctor.
Adversarial 
a few months, i guess .

Input 
so i had the doctors test sammy ’s response to conditioning .
Vanilla-SEQ2SEQ 
sammy wrote the test sammy wrote the test .
Adversarial 
so he took the pills .

Input 
they didn ’t have much success with this problem commander .
Vanilla-SEQ2SEQ 
they ’re not the only ones who have been in the system .
Adversarial
 can we find someone else ?
```

Figure 26.7 Sample responses generated by a SEQ2SEQ model trained either with a vanilla maximum likelihood objective, or adversarially trained to produce sentences that are hard for an adversary to distinguish from human sentences (Li et al., 2017).

그림 26.7 바닐라 최대 가능성 목표로 훈련되거나 적을 인간 문장과 구별하기 어려운 문장을 생성하도록 적법하게 훈련 된 SEQ2SEQ 모델에 의해 생성 된 샘플 응답 (Li et al., 2017).

#### Evaluating Chatbots

Chatbots are generally evaluated by humans. The slot-filling evaluations used for task-based dialogue (Section 26.5) aren’t appropriate for this task (Artstein et al., 2009), and word-overlap metrics like BLEU for comparing a chatbot’s response to a human response turn out to correlate very poorly with human judgments (Liu et al., 2016). BLEU performs poorly because there are so many possible responses to any given turn; word-overlap metrics work best when the space of responses is small and lexically overlapping, as is the case in machine translation. While human evaluation is therefore required for evaluating chatbots, there are beginning to be models for automatic evaluation. The ADEM (Lowe et al., 2017a) classifier is trained on a set of responses labeled by humans with how appropriate they are, and learns to predict this label from the dialogue context and the words in the system response. Another paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and adversarial evaluation Vinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a "Turing-like" evaluator classifier to distinguish between human-generated responses and machine-generated responses. The more successful a response generation system is at fooling this evaluator, the better the system.

챗봇은 일반적으로 인간에 의해 평가됩니다. 작업 기반 대화 (섹션 26.5)에 사용 된 슬롯 채우기 평가는이 작업에 적합하지 않으며 (Artstein et al., 2009), 챗봇의 응답과 인간의 응답을 비교하기위한 BLEU와 같은 단어 오버랩 메트릭은 다음과 같습니다. 인간의 판단과는 매우 밀접한 관련이있다 (Liu et al., 2016). BLEU는 주어진 턴에 너무 많은 응답이 있기 때문에 성능이 좋지 않습니다. 단어 오버랩 메트릭은 기계 번역에서와 같이 응답 공간이 작고 어휘 적으로 겹치는 경우에 가장 효과적입니다. 따라서 챗봇을 평가하려면 사람의 평가가 필요하지만 자동 평가를위한 모델이 시작되었습니다. ADEM (Lowe et al., 2017a) 분류기는 인간이 얼마나 적절한지를 표시 한 일련의 응답에 대해 학습하고 대화 컨텍스트와 시스템 응답의 단어에서이 레이블을 예측하는 방법을 배웁니다. 또 다른 패러다임은 튜링 테스트에서 영감을 얻은 적대적 평가 (Bowman et al. 2016, Kannan 및 adversarial evaluation Vinyals 2016, Li et al. 2017)입니다. 아이디어는 인간 생성 응답과 기계 생성 응답을 구별하기 위해 "Turing-like"평가자 분류기를 훈련시키는 것입니다. 응답 생성 시스템이이 평가자를 속 일수록 시스템이 향상됩니다.

## GUS: Simple Frame-based Dialogue Systems

We turn now to task-based dialogue, in which a dialogue system has the goal of helping a user solve some task like making an airplane reservation or buying a prodGUS uct. In this section we introduce the very simple GUS architecture for task-based dialogue. Although it is quite old, (it was first introduced in 1977 in the influential GUS system for travel planning (Bobrow et al., 1977)) the GUS architecture has been astonishingly long-lived and underlies most if not all modern commercial digital assistants. Figure 26.8 shows a conversation with the original GUS system. All modern task-based dialogue systems, whether the simple GUS architecture dialogue state we describe here, or the more sophisticated dialogue state architectures we turn to frame in the following section, are based around frames. A frame is a kind of knowledge structure representing the kinds of intentions the system can extract from user senslot tences, and consists of a collection of slots, each of which can take a set of possible values. Together this set of frames is sometimes called a domain ontology.

이제 작업 기반 대화로 넘어가겠습니다. 대화 시스템은 사용자가 비행기 예약 또는 제품 구매와 같은 일부 작업을 해결하도록 돕는 목표를 가지고 있습니다. 이 섹션에서는 작업 기반 대화를위한 매우 간단한 GUS 아키텍처를 소개합니다. 비록 오래되었지만 (여행 계획을위한 영향력있는 GUS 시스템에서 1977 년에 처음 소개 되었음 (Bobrow et al., 1977)) GUS 아키텍처는 놀랍도록 오래 지속되어 왔으며 현대의 모든 상업 디지털 어시스턴트의 대부분은 아닙니다. 그림 26.8은 원래 GUS 시스템과의 대화를 보여줍니다. 여기에서 설명하는 간단한 GUS 아키텍처 대화 상태 또는 다음 섹션에서 프레임으로 전환 할보다 정교한 대화 상태 아키텍처에 관계없이 모든 최신 작업 기반 대화 시스템은 프레임을 기반으로합니다. 프레임은 시스템이 사용자 감지 슬롯 시제에서 추출 할 수있는 의도의 종류를 나타내는 일종의 지식 구조이며 각 슬롯은 가능한 값 집합을 취할 수있는 슬롯 모음으로 구성됩니다. 이 프레임 세트를 함께 도메인 온톨로지라고합니다.

```
GUS: Hello. My name is GUS. I can help you plan a simple trip by air. Where do you
want to go ?
Client: I want to go to San Diego on May 28
GUS: What time do you want to leave?
Client: I must be in San Diego before 10 am
GUS: Would you like P.S.A. flight 102 that arrives at 9.15 am ?
Client: What is the next flight ?
GUS: Air California flight 310 that leaves at 8.30am and arrives at 10.00am
Client: I’ll take the first one
GUS: In what name should I make a reservation, please?
Client: Dan Bobrow
GUS: I have confirmed the following flight: P.S.A. flight 102 on Wednesday May 28
from San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am.
GUS: What date do you want to return on ?
Client: On Friday in the evening.
GUS: Would you like the flight that leaves at 7.45 pm ?
Client: That’s fine.
GUS: I have confirmed the following flight: P.S.A. flight 307 on Friday May 30 from
San Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank you
for calling. Goodbye
```
Figure 26.8 The travel domain: A transcript of an actual dialogue with the GUS system of Bobrow et al. (1977). P.S.A. and Air California were airlines of that period.

The set of slots in a task-based dialogue frame specifies what the system needs to know, and the filler of each slot is constrained to values of a particular semantic type. In the travel domain, for example, a slot might be of type city (hence take on values like San Francisco, or Hong Kong) or of type date, airline, or time.

여행 영역 : Bobrow et al.의 GUS 시스템과의 실제 대화 내용. (1977). 추신 Air California는 그 당시의 항공사였습니다.

작업 기반 대화 프레임의 슬롯 세트는 시스템이 알아야 할 사항을 지정하며 각 슬롯의 필러는 특정 의미 유형의 값으로 제한됩니다. 예를 들어, 여행 도메인에서 슬롯은 도시 유형 (샌프란시스코 또는 홍콩과 같은 값을 가짐)이거나 날짜, 항공사 또는 시간 유형일 수 있습니다.

```
Slot Type Question Template
ORIGIN CITY city "From what city are you leaving?"
DESTINATION CITY city "Where are you going?"
DEPARTURE TIME time "When would you like to leave?"
DEPARTURE DATE date "What day would you like to leave?"
ARRIVAL TIME time "When do you want to arrive?"
ARRIVAL DATE date "What day would you like to arrive?"
```
Figure 26.9 A frame in a frame-based dialogue system, showing the type of each slot and a question used to fill the slot.

Types in GUS, as in modern frame-based dialogue agents, have hierarchical structure; for example the date type in GUS is itself a frame with slots with types like integer or members of sets of weekday names:

현대의 프레임 기반 대화 에이전트에서와 같이 GUS의 유형은 계층 구조를 갖습니다. 예를 들어 GUS의 날짜 유형은 정수 또는 요일 이름 집합의 멤버와 같은 유형의 슬롯이있는 프레임입니다.

```
DATE
    MONTH:NAME YEAR:INTEGER DAY:(BOUNDED-INTEGER 1 31)
    WEEKDAY:(MEMBER (Sunday Monday Tuesday Wednesday Thursday Friday Saturday))
        
```

### 26.3.1 Control structure for frame-based dialogue

The control architecture for frame-based dialogue systems, used in various forms in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assistant, is designed around the frame. The system’s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight).  To do this, the system asks questions of the user (using pre-specified question templates associated with each slot of each frame, as shown in Fig. 26.9), filling any slot that the user specifies (we’ll describe how slot-filling works in the next section). If a user’s response fills multiple slots, like the following:

Apple의 Siri, Amazon의 Alexa 및 Google Assistant와 같은 최신 시스템에서 다양한 형태로 사용되는 프레임 기반 대화 시스템의 제어 아키텍처는 프레임 주위에 설계되었습니다. 시스템의 목표는 사용자가 원하는 필러로 프레임의 슬롯을 채운 다음 사용자에 대한 관련 작업 (질문에 응답하거나 항공편을 예약)을 수행하는 것입니다. 이를 위해 시스템은 사용자의 질문을하고 (그림 26.9에 표시된 것처럼 각 프레임의 각 슬롯과 관련된 미리 지정된 질문 템플릿을 사용하여) 사용자가 지정한 슬롯을 채 웁니다 (슬롯 채우기 방법을 설명합니다) 다음 섹션에서 작동합니다). 사용자의 응답이 다음과 같이 여러 슬롯을 채우는 경우 :

```
(26.3) I want a flight from San Francisco to Denver one way leaving after five
p.m. on Tuesday.
```

the system fills all the relevant slots, and then continues asking questions to fill the remaining slots, skipping questions associated with filled slots. The GUS architecture also has condition-action rules attached to slots. For example, a rule attached to the DESTINATION slot for the plane booking frame, once the user has specified the destination, might automatically enter that city as the default StayLocation for the related hotel booking frame. Or if the user specifies the DESTINATION DAY for a short trip the system could automatically enter the ARRIVAL DAY. Many domains require multiple frames. Besides frames for car or hotel reservations, we might need frames with general route information (for questions like Which airlines fly from Boston to San Francisco?), or information about airfare practices (for questions like Do I have to stay a specific number of days to get a decent airfare?). The system must be able to disambiguate which slot of which frame a given input is supposed to fill and then switch dialogue control to that frame. Because of this need to dynamically switch control, the GUS architecture is a production rule system. Different types of inputs cause different productions to fire, each of which can flexibly fill in different frames. The production rules can then switch control according to factors such as the user’s input and some simple dialogue history like the last question that the system asked. Once the system has enough information it performs the necessary action (like querying a database of flights) and returns the result to the user.

시스템은 모든 관련 슬롯을 채운 다음 나머지 슬롯을 채울 질문을 계속하여 채워진 슬롯과 관련된 질문을 건너 뜁니다. GUS 아키텍처에는 슬롯에 연결된 조건 동작 규칙도 있습니다. 예를 들어, 비행기 예약 프레임의 DESTINATION 슬롯에 부착 된 규칙은 사용자가 목적지를 지정하면 관련 호텔 예약 프레임의 기본 StayLocation으로 해당 도시를 자동으로 입력 할 수 있습니다. 또는 사용자가 짧은 여행에 대해 DESTINATION DAY를 지정하면 시스템은 자동으로 도착 일을 입력 할 수 있습니다. 많은 도메인에는 여러 개의 프레임이 필요합니다. 자동차 또는 호텔 예약 용 프레임 외에 일반 노선 정보 (보스턴에서 샌프란시스코로 운항하는 항공사와 같은 질문의 경우) 또는 항공료 관행에 대한 정보 (특정 기간 동안 머물러야하는 경우)가있는 프레임이 필요할 수 있습니다. 알맞은 항공료를 얻으려면?). 시스템은 주어진 입력이 어느 프레임의 어느 슬롯에서 채워질지를 명확하게하고 대화 제어를 해당 프레임으로 전환 할 수 있어야합니다. 이러한 제어를 동적으로 전환해야하므로 GUS 아키텍처는 프로덕션 규칙 시스템입니다. 다른 유형의 입력으로 인해 서로 다른 제작물이 발생하여 각기 다른 프레임을 유연하게 채울 수 있습니다. 생산 규칙은 사용자 입력과 같은 요소 및 시스템이 마지막으로 질문 한 것과 같은 간단한 대화 기록과 같은 요소에 따라 제어를 전환 할 수 있습니다. 시스템에 충분한 정보가 있으면 필요한 조치 (예 : 비행 데이터베이스 조회)를 수행하고 결과를 사용자에게 리턴합니다.

### 26.3.2 Natural language understanding for filling slots in GUS

The goal of the natural language understanding component in the frame-based architecture is to extract three things from the user’s utterance. The first task is domain classification: is this user for example talking about airlines, programming an alarm clock, or dealing with their calendar? Of course this 1-of-n classification tasks is unnecessary for single-domain systems that are focused on, say, only calendar management, but multi-domain dialogue systems are the modern standard. The second is user intent determination: what general task or goal is the user trying to accom- intent determination plish? For example the task could be to Find a Movie, or Show a Flight, or Remove slot filling a Calendar Appointment. Finally, we need to do slot filling: extract the particular slots and fillers that the user intends the system to understand from their utterance with respect to their intent. From a user utterance like this one:

프레임 기반 아키텍처에서 자연어 이해 구성 요소의 목표는 사용자의 발화에서 세 가지를 추출하는 것입니다. 첫 번째 작업은 도메인 분류입니다. 예를 들어이 사용자는 항공사에 대해 이야기하거나 알람 시계를 프로그래밍하거나 캘린더를 처리합니까? 물론이 1-n-n 분류 작업은 달력 관리에만 중점을 둔 단일 도메인 시스템에는 필요하지 않지만 다중 도메인 대화 시스템은 현대 표준입니다. 두 번째는 사용자 의도 결정입니다. 사용자가 어떤 일반 작업 또는 목표를 달성하려고합니까? 예를 들어 작업은 영화를 찾거나 비행기를 보여 주거나 달력 약속을 채우는 슬롯을 제거하는 것입니다. 마지막으로, 슬롯 채우기를 수행해야합니다. 사용자가 의도와 관련하여 발언에서 시스템이 이해하려는 특정 슬롯과 필러를 추출합니다. 이와 같은 사용자 발화에서 :

```
Show me morning flights from Boston to San Francisco on Tuesday
a system might want to build a representation like:
DOMAIN: AIR-TRAVEL
INTENT: SHOW-FLIGHTS
ORIGIN-CITY: Boston
ORIGIN-DATE: Tuesday
ORIGIN-TIME: morning
DEST-CITY: San Francisco
```

while an utterance like

```
Wake me tomorrow at 6
```

should give an intent like this:

```
DOMAIN: ALARM-CLOCK
INTENT: SET-ALARM
TIME: 2017-07-01 0600-0800
```

The slot-filling method used in the original GUS system, and still quite common in industrial applications, is to use handwritten rules, often as part of the conditionaction rules attached to slots or concepts. For example we might just define a regular expression for recognizing the SET-ALARM intent:

원래 GUS 시스템에서 사용되며 산업 응용 프로그램에서 여전히 일반적으로 사용되는 슬롯 채우기 방법은 종종 슬롯 또는 개념에 첨부 된 조건 작업 규칙의 일부로 필기 규칙을 사용하는 것입니다. 예를 들어 SET-ALARM 의도를 인식하기위한 정규식을 정의 할 수 있습니다.


```
wake me (up) | set (the|an) alarm | get me up
```

Rule-based research systems like the Phoenix system (Ward and Issar, 1994) consist of large hand-designed semantic grammars with thousands of rules. A semantic grammar semantic grammar is a context-free grammar in which the left-hand side of each rule corresponds to the semantic entities being expressed (i.e., the slot names) as in the following fragment:

피닉스 시스템 (Ward and Issar, 1994)과 같은 규칙 기반 연구 시스템은 수천 개의 규칙을 가진 수작업으로 설계된 의미 론적 문법으로 구성됩니다. 시맨틱 문법 시맨틱 문법은 문맥이없는 문법으로, 각 규칙의 왼쪽이 다음과 같이 표현되는 의미 론적 엔티티 (즉, 슬롯 이름)에 해당합니다.

( 생략 )

## 26.4 The Dialogue-State Architecture

Modern research systems for task-based dialogue are based on a more sophisticated version of the frame-based architecture called the dialogue-state or belief-state architecture. Figure 26.11 shows the six components of a typical dialogue-state system. The speech recognition and synthesis components deal with spoken language processing; we’ll return to them in Chapter 28. For the rest of this chapter we therefore consider the other four components, which are part of both spoken and textual dialogue systems. These four components are more complex than in the simple GUS systems. For example, like the GUS systems, the dialogue-state architecture has an NLU component to extract slot fillers from the user’s utterance, but generally using machine learning rather than rules. The dialogue state tracker maintains the current state of the dialogue (which include the user’s most recent dialogue act, plus the entire set of slot-filler constraints the user has expressed so far). The dialogue policy decides what the system should do or say next. The dialogue policy in GUS was simple: ask questions until the frame was full and then report back the results of some database query. But a more sophisticated dialogue policy can help a system decide when to answer the user’s questions, when to instead ask the user a clarification question, when to make a suggestion, and so on. Finally, dialogue state systems have a natural language generation component. In GUS, the sentences that the generator produced were all from pre-written templates. But a more sophisticated generation component can condition on the exact context to produce turns that seem much more natural. As of the time of this writing, most commercial system are architectural hybrids, based on GUS architecture augmented with some dialogue-state components, but there are a wide variety of dialogue-state systems being developed in research labs.

작업 기반 대화를위한 최신 연구 시스템은 대화 상태 또는 신념 상태 아키텍처라고하는 더 정교한 버전의 프레임 기반 아키텍처를 기반으로합니다. 그림 26.11은 전형적인 대화 상태 시스템의 6 가지 구성 요소를 보여줍니다. 음성 인식 및 합성 구성 요소는 음성 언어 처리를 처리합니다. 이 장의 나머지 부분에서는 음성 및 텍스트 대화 시스템의 일부인 다른 네 가지 구성 요소를 고려합니다. 이 4 가지 구성 요소는 단순한 GUS 시스템보다 복잡합니다. 예를 들어, GUS 시스템과 같이 대화 상태 아키텍처에는 사용자 발언에서 슬롯 필러를 추출하기위한 NLU 구성 요소가 있지만 일반적으로 규칙보다는 기계 학습을 사용합니다. 대화 상태 추적기는 대화의 현재 상태 (사용자의 최근 대화 행위와 사용자가 지금까지 표현한 전체 슬롯 필러 제약 조건 포함)를 유지합니다. 대화 정책은 시스템이 다음에해야 할 일이나 할 말을 결정합니다. GUS의 대화 정책은 간단했습니다. 프레임이 가득 찰 때까지 질문 한 다음 일부 데이터베이스 쿼리 결과를 다시보고하십시오. 그러나보다 정교한 대화 정책을 통해 시스템은 사용자의 질문에 대답 할시기, 사용자에게 설명을 요구할시기, 제안 할시기 등을 결정하는 데 도움이됩니다. 마지막으로, 대화 상태 시스템에는 자연어 생성 구성 요소가 있습니다. GUS에서 생성기가 생성 한 문장은 모두 미리 작성된 템플릿에서 나온 것입니다. 그러나보다 정교한 세대 구성 요소는 훨씬 자연스러운 것처럼 보이는 턴을 만들기 위해 정확한 상황에 따라 달라질 수 있습니다. 이 글을 쓰는 시점에서 대부분의 상용 시스템은 일부 대화 상태 구성 요소로 보강 된 GUS 아키텍처를 기반으로하는 건축 하이브리드이지만 연구 실험실에서 개발중인 다양한 대화 상태 시스템이 있습니다.

### 26.4.1 Dialogue Acts

dialogue acts Dialogue-state systems make use of dialogue acts. Dialogue acts represent the interactive function of the turn or sentence, combining the idea of speech acts and grounding into a single representation. Different types of dialogue systems require labeling different kinds of acts, and so the tagset—defining what a dialogue act is exactly— tends to be designed for particular tasks. Figure 26.12 shows a tagset for a restaurant recommendation system, and Fig. 26.13 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue acts, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable.

대화 행위 대화 상태 시스템은 대화 행위를 사용합니다. 대화 행위는 연설 행위의 개념과 접지를 단일 표현으로 결합하여 턴 또는 문장의 대화식 기능을 나타냅니다. 다양한 유형의 대화 시스템에는 다른 유형의 동작에 레이블을 지정해야하므로 대화 동작을 정확히 정의하는 태그 세트는 특정 작업에 맞게 설계되는 경향이 있습니다. 그림 26.12는 식당 추천 시스템에 대한 태그 세트를 보여주고, 그림 26.13은 HIS 시스템의 샘플 대화에 레이블을 붙인 태그를 보여줍니다 (Young et al., 2010). 이 예는 또한 통신되는 슬롯 필러 인 각 대화 행위의 내용을 보여줍니다. 따라서 사용자는 박물관 근처에서 이탈리아 음식을 원한다고 시스템에 알리거나 가격이 합리적이라고 시스템에 확인하십시오.

```
Tag Sys User Description
HELLO(a = x,b = y,...) X X Open a dialogue and give info a = x,b = y,...
INFORM(a = x,b = y,...) X X Give info a = x,b = y,...
REQUEST(a,b = x,...) X X Request value for a given b = x,...
REQALTS(a = x,...) χ X Request alternative with a = x,...
CONFIRM(a = x,b = y,...) X X Explicitly confirm a = x,b = y,...
CONFREQ(a = x,...,d) X χ Implicitly confirm a = x,... and request value of d
SELECT(a = x,a = y) X χ Implicitly confirm a = x,... and request value of d
AFFIRM(a = x,b = y,...) X X Affirm and give further info a = x,b = y,...
NEGATE(a = x) χ X Negate and give corrected value a = x
DENY(a = x) χ X Deny that a = x
BYE() X X Close a dialogue
```
Figure 26.12 Dialogue acts used by the HIS restaurant recommendation system of Young et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and user inputs, respectively.

그림 26.12 Young et al.의 HIS 식당 추천 시스템이 사용하는 대화 행위. (2010). Sys 및 User 열은 각각 시스템 출력 및 사용자 입력으로 유효한 동작을 나타냅니다.

```
Utterance Dialogue act
U: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)
S: You are looking for a restaurant. What
type of food do you like?
confreq(type = restaurant, food)
U: I’d like an Italian somewhere near the
museum.
inform(food = Italian, near=museum)
S: Roma is a nice Italian restaurant near
the museum.
inform(name = "Roma", type = restaurant,
food = Italian, near = museum)
U: Is it reasonably priced? confirm(pricerange = moderate)
S: Yes, Roma is in the moderate price
range.
affirm(name = "Roma", pricerange =
moderate)
U: What is the phone number? request(phone)
S: The number of Roma is 385456. inform(name = "Roma", phone = "385456")
U: Ok, thank you goodbye. bye()
```

Figure 26.13 A sample dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 26.12.

그림 26.13 Young 등의 HIS 시스템의 샘플 대화. (2010)은 대화를 사용하여 그림 26.12에서 작동합니다.

### 26.4.2 Slot Filling

The task of slot-filling, and the simpler tasks of domain and intent classification, are special cases of the task of supervised semantic parsing discussed in Chapter 17, in which we have a training set that associates each sentence with the correct set of slots, domain, and intent. A simple method is to train a a sequence model to map from input words representation to slot fillers, domain and intent. For example given the sentence:

슬롯 채우기 작업과 도메인 및 의도 분류 작업의 간단한 작업은 17 장에서 설명한 감독 된 의미 구문 분석 작업의 특수한 경우이며, 각 문장을 올바른 슬롯 세트와 연관시키는 훈련 세트가 있습니다. 도메인 및 의도. 간단한 방법은 입력 단어 표현에서 슬롯 필러, 도메인 및 의도에 매핑하도록 시퀀스 모델을 학습시키는 것입니다. 예를 들어 문장이 주어진 경우 :

```
I want to fly to San Francisco on Monday afternoon please
```

we compute a sentence representation, for example by passing the sentence through a contextual embedding network like BERT. The resulting sentence representation can be passed through a feedforward layer and then a simple 1-of-N classifier to determine that the domain is AIRLINE and and the intent is SHOWFLIGHT. IOB Our training data is sentences paired with sequences of IOB labels:

예를 들어 문장을 BERT와 같은 문맥 삽입 네트워크를 통해 전달하여 문장 표현을 계산합니다. 결과 문장 표현은 피드 포워드 레이어를 통과 한 다음 간단한 1-N 분류기를 통해 도메인이 AIRLINE이고 의도가 SHOWFLIGHT인지 확인할 수 있습니다. IOB 교육 데이터는 일련의 IOB 레이블과 쌍을 이루는 문장입니다.

```
O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O
I want to fly to San Francisco on Monday afternoon please
```

Recall from Chapter 18 that in IOB tagging we introduce a tag for the beginning (B) and inside (I) of each slot label, and one for tokens outside (O) any slot label. The number of tags is thus 2n+1 tags, where n is the number of slots.  Fig. 26.14 shows the architecture. The input is a series of words w1...wn, which is passed through a contextual embedding model to get contextual word representations. This is followed by a feedforward layer and a softmax at each token position over possible IOB tags, with the output is a series of IOB tags s1...sn. We can also combine the domain-classification and intent-extraction tasks with slot-filling simply by adding a domain concatenated with an intent as the desired output for the final EOS token.

18 장에서 IOB 태깅에서는 각 슬롯 레이블의 시작 (B)과 내부 (I)에 대한 태그와 슬롯 레이블 외부 (O)에 대한 태그를 소개합니다. 따라서 태그 수는 2n + 1 태그이며 여기서 n은 슬롯 수입니다. 그림 26.14는 아키텍처를 보여줍니다. 입력은 일련의 단어 w1 ... wn이며, 상황 별 단어 표현을 얻기 위해 상황 별 임베딩 모델을 통과합니다. 그 다음에는 가능한 IOB 태그에 대한 각 토큰 위치에서 피드 포워드 계층과 소프트 맥스가 나오며 출력은 일련의 IOB 태그 s1 ... sn입니다. 또한 최종 EOS 토큰의 원하는 출력으로 인 텐트가 연결된 도메인을 추가하여 도메인 분류 및 인 텐트 추출 작업을 슬롯 채우기와 결합 할 수 있습니다.

```
```

Figure 26.14 A simple architecture for slot filling, mapping the words in the input through contextual embeddings like BERT to an output classifier layer (which can be linear or something more complex), followed by softmax to generate a series of IOB tags (and including a final state consisting of a domain concatenated with an intent).

그림 26.14 BERT와 같은 컨텍스트 임베딩을 통해 입력의 단어를 출력 분류기 레이어 (선형 적이거나 더 복잡한 것일 수 있음)에 매핑 한 다음 softmax를 사용하여 일련의 IOB 태그를 생성하고 의도와 연결된 도메인으로 구성된 최종 상태).

Once the sequence labeler has tagged the user utterance, a filler string can be extracted for each slot from the tags (e.g., "San Francisco"), and these word strings can then be normalized to the correct form in the ontology (perhaps the airport code ‘SFO’). This normalization can take place by using homonym dictionaries (specifying, for example, that SF, SFO, and San Francisco are the same place). In industrial contexts, machine learning-based systems for slot-filling are often bootstrapped from GUS-style rule-based systems in a semi-supervised learning manner. A rule-based system is first built for the domain, and a test-set is carefully labeled. As new user utterances come in, they are paired with the labeling provided by the rule-based system to create training tuples. A classifier can then be trained on these tuples, using the test-set to test the performance of the classifier against the rule-based system. Some heuristics can be used to eliminate errorful training tuples, with the goal of increasing precision. As sufficient training samples become available the resulting classifier can often outperform the original rule-based system (Suendermann et al., 2009), although rule-based systems may still remain higherprecision for dealing with complex cases like negation.

시퀀스 라벨러가 사용자 발화에 태그를 지정하면 필러 문자열을 태그에서 각 슬롯에 대해 추출 할 수 있으며 (예 : "샌프란시스코") 이러한 단어 문자열을 온톨로지 (아마도 공항)에서 올바른 형식으로 정규화 할 수 있습니다. 코드 'SFO'). 이 정규화는 homonym 사전을 사용하여 수행 할 수 있습니다 (예 : SF, SFO 및 San Francisco가 동일한 장소임을 지정). 산업 상황에서 슬롯 채우기를위한 기계 학습 기반 시스템은 종종 반 감독 학습 방식으로 GUS 스타일 규칙 기반 시스템에서 부트 스트랩됩니다. 도메인에 대한 규칙 기반 시스템이 먼저 구축되고 테스트 세트에주의하여 레이블이 지정됩니다. 새로운 사용자 발언이 들어 오면, 규칙 기반 시스템에서 제공하는 라벨과 쌍을 이루어 훈련 튜플을 만듭니다. 그런 다음 테스트 세트를 사용하여 규칙 기반 시스템에 대한 분류기의 성능을 테스트하여 이러한 튜플에 대한 분류기를 학습 할 수 있습니다. 일부 휴리스틱은 정확성을 높이기 위해 오류가있는 트레이닝 튜플을 제거하는 데 사용할 수 있습니다. 충분한 훈련 샘플을 이용할 수있게되면 결과 분류기는 종종 원래의 규칙 기반 시스템보다 성능이 우수 할 수 있지만 (Suendermann et al., 2009), 규칙 기반 시스템은 부정과 같은 복잡한 사례를 처리하는 데 여전히 높은 정밀도를 유지할 수 있습니다.

### 26.4.3 Dialogue State Tracking

The job of the dialogue-state tracker is to determine both the current state of the frame (the fillers of each slot), as well as the user’s most recent dialogue act. The dialogue-state thus includes more than just the slot-fillers expressed in the current sentence; it includes the entire state of the frame at this point, summarizing all of the user’s constraints. The following example from Mrksiˇ c et al. (2017) ´ shows the required output of the dialogue state tracker after each turn:

대화 상태 추적기의 작업은 프레임의 현재 상태 (각 슬롯의 필러)와 사용자의 가장 최근 대화 동작을 결정하는 것입니다. 따라서 대화 상태는 현재 문장에서 표현 된 슬롯 필러 이상의 것을 포함합니다. 이 시점에서 프레임의 전체 상태가 포함되어 모든 사용자의 제약 조건을 요약합니다. Mrksiˇ c et al. (2017)는 각 턴 후 대화 상태 추적기의 필요한 출력을 보여줍니다.

```
User: I’m looking for a cheaper restaurant
inform(price=cheap)
System: Sure. What kind - and where?
User: Thai food, somewhere downtown
inform(price=cheap, food=Thai, area=centre)
System: The House serves cheap Thai food
User: Where is it?
inform(price=cheap, food=Thai, area=centre); request(address)
System: The House is at 106 Regent Street
```

Since dialogue acts place some constraints on the slots and values, the tasks of dialogue-act detection and slot-filling are often performed jointly. Consider the task of determining that

대화 동작은 슬롯과 값에 일부 제약을두기 때문에 대화 동작 감지와 슬롯 채우기 작업은 종종 공동으로 수행됩니다. 그 결정 작업을 고려

```
I’d like Cantonese food near the Mission District
```

has the structure

```
inform(food=cantonese,area=mission).
```

Dialogue act interpretation—in this example choosing inform from the set of dialogue acts for this task—is done by supervised classification trained on handlabeled dialog acts, predicting the dialogue act tag based on embeddings representing the current input sentence and the prior dialogue acts. The simplest dialogue state tracker might just take the output of a slot-filling sequence-model (Section 26.4.2) after each sentence. Alternatively, a more complex model can make use of the reading-comprehension architectures from Chapter 25. For example the model of Gao et al. (2019) trains a classifier for each slot to decide whether its value is being changed in the current sentence or should be carried over from the previous sentences. If the slot value is being changed, a span-prediction model is used to predict the start and end of the span with the slot filler.

대화 행위 해석 (이 예에서이 작업에 대한 대화 행위 세트에서 정보를 선택하는 것)은 현재 입력 된 문장과 이전 대화 행위를 나타내는 임베드를 기반으로 대화 행위 태그를 예측하는 핸드 라벨 된 대화 행위에 대해 훈련 된 감독 분류에 의해 수행됩니다. 가장 간단한 대화 상태 추적기는 각 문장마다 슬롯 채우기 시퀀스 모델 (섹션 26.4.2)을 출력 할 수 있습니다. 또는 더 복잡한 모델은 25 장의 독해 구조를 사용할 수 있습니다. 예를 들어 Gao et al. (2019)는 각 문장에 대해 분류자를 훈련시켜 현재 문장에서 값이 변경되는지 또는 이전 문장에서 이월되어야하는지 결정합니다. 슬롯 값이 변경되는 경우, 스팬 예측 모델을 사용하여 슬롯 필러로 스팬의 시작과 끝을 예측합니다.

**A special case: detecting correction acts**

Some dialogue acts are important because of their implications for dialogue control. If a dialogue system misrecognizes or misunderstands an utterance, the user will generally correct the error by repeating or reformulating the utterance. Detecting these user correction acts is therefore quite important. Ironically, it turns out that user correction acts corrections are actually harder to recognize than normal sentences! In fact, corrections in one early dialogue system (the TOOT system) had double the ASR word error rate of non-corrections (Swerts et al., 2000)! One reason for this is that speakers sometimes use a specific prosodic style for corrections called hyperarticulation, hyperarticulation in which the utterance contains exaggerated energy, duration, or F0 contours, such as I said BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Even when they are not hyperarticulating, users who are frustrated seem to speak in a way that is harder for speech recognizers (Goldberg et al., 2003). What are the characteristics of these corrections? User corrections tend to be either exact repetitions or repetitions with one or more words omitted, although they may also be paraphrases of the original utterance. (Swerts et al., 2000). Detecting these reformulations or correction acts can be part of the general dialogue act detection classifier. Alternatively, because the cues to these acts tend to appear in different ways than for simple acts (like INFORM or request, we can make use of features orthogonal to simple contextual embedding features; some typical features are shown below (Levow 1998, Litman et al. 1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015):

일부 대화는 대화 제어에 영향을주기 때문에 중요합니다. 대화 시스템이 발화를 잘못 인식하거나 오해하는 경우, 사용자는 일반적으로 발화를 반복하거나 재구성하여 오류를 수정합니다. 따라서 이러한 사용자 수정 행위를 감지하는 것이 매우 중요합니다. 아이러니하게도, 사용자 수정 행위 수정은 실제 문장보다 인식하기가 어렵다는 것이 밝혀졌습니다! 실제로, 하나의 초기 대화 시스템 (TOOT 시스템)에서의 정정은 ASR 비 오차 정정 오류율의 두 배를 가졌다 (Swerts et al., 2000)! 그 이유 중 하나는 스피커가 때때로 과음 절 (hyperarticulation)이라고하는 교정을 위해 특정 프로 소딕 스타일을 사용하기 때문입니다. 1992, Levow 1998, Hirschberg et al. 2001). 그들이 과음 절이 아니더라도, 좌절 한 사용자들은 음성 인식 자들에게 더 어려운 방식으로 말하는 것처럼 보입니다 (Goldberg et al., 2003). 이 수정의 특징은 무엇입니까? 사용자 정정은 하나 이상의 단어가 생략 된 정확한 반복 또는 반복 인 경향이 있지만, 원래 발화의 역설 일 수도 있습니다. (Swerts et al., 2000). 이러한 개혁이나 수정 행위를 탐지하는 것은 일반적인 대화 행위 탐지 분류기의 일부가 될 수 있습니다. 또는 이러한 행위에 대한 신호는 간단한 행위 (INFORM 또는 요청과 같은)와 다른 방식으로 나타나는 경향이 있기 때문에 간단한 문맥 삽입 기능과 직교하는 특징을 사용할 수 있습니다. 일부 전형적인 특징은 다음과 같습니다 (Levow 1998, Litman et al 1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015) :

```
features examples
lexical words like "no", "correction", "I don’t", or even swear words, utterance length
semantic similarity (word overlap or embedding cosine) between the candidate correction act and the user’s prior utterance
phonetic phonetic overlap between the candidate correction act and the user’s prior utterance (i.e. "WhatsApp" may be incorrectly recognized as "What’s up")
prosodic hyperarticulation, increases in F0 range, pause duration, and word duration,
generally normalized by the values for previous sentences
ASR ASR confidence, language model probability
```

### 26.4.4 Dialogue Policy

The goal of the dialogue policy is to decide what action the system should take next, that is, what dialogue act to generate. More formally, at turn i in the conversation we want to predict which action Ai to take, based on the entire dialogue state. The state could mean the entire sequence of dialogue acts from the system (A) and from the user (U), in which case the task would be to compute:

대화 정책의 목표는 시스템이 다음에 수행해야 할 작업, 즉 어떤 대화가 생성되도록 결정하는 것입니다. 보다 공식적으로, 대화에서 i는 전체 대화 상태를 기반으로 Ai가 수행 할 조치를 예측하려고합니다. 상태는 시스템 (A)과 사용자 (U)의 전체 대화 동작 순서를 의미 할 수 있으며,이 경우 작업은 다음과 같이 계산됩니다.

```
Aˆ
i = argmax
Ai∈A
P(Ai
|(A1,U1,...,Ai−1,Ui−1) (26.6)
```

We can simplify this by maintaining as the dialogue state mainly just the set of slot-fillers that the user has expressed, collapsing across the many different conversational paths that could lead to the same set of filled slots. Such a policy might then just condition on the current dialogue state as represented just by the current state of the frame Framei (which slots are filled and with what) and the last turn by the system and user:

대화 상태를 주로 사용자가 표현한 일련의 슬롯 필러 세트로 유지하여 동일한 채워진 슬롯 세트로 이어질 수있는 다양한 대화 경로를 가로 질러 축소함으로써이를 단순화 할 수 있습니다. 이러한 정책은 프레임 Framei의 현재 상태 (슬롯이 채워지고 무엇으로 채워짐)와 시스템 및 사용자가 마지막으로 표시 한대로 현재 대화 상태를 조건으로 할 수 있습니다.

```
Aˆ
i = argmax
Ai∈A
P(Ai
|Framei−1,Ai−1,Ui−1) (26.7)
```

These probabilities can be estimated by a neural classifier using neural representations of the slot fillers (for example as spans) and the utterances (for example as sentence embeddings computed over contextual embeddings) More sophisticated models train the policy via reinforcement learning. To decide which action to take, a reinforcement learning system gets a reward at the end of the dialogue, and uses that reward to train a policy to take actions. For example in the movie-recommendation dialogue system of Fazel-Zarandi et al. (2017), the action space has only three actions: EXECUTE, CONFIRM, and ELICIT. The EXECUTE sends a query to the database and answers the user’s question, CONFIRM clarifies the intent or slot with the users (e.g., "Do you want movies directed by Christopher Nolan?") while ELICIT asks the user for missing information (e.g., "Which movie are you talking about?"). The system gets a large positive reward if the dialogue system terminates with the correct slot representation at the end, a large negative reward if the slots are wrong, and a small negative reward for confirmation and elicitation questions to keep the system from re-confirming everything.

이러한 확률은 슬롯 필러 (예 : 스팬) 및 발화 (예 : 문맥 임베딩을 통해 계산 된 문장 임베딩)의 신경 표현을 사용하여 신경 분류 기준으로 추정 할 수 있습니다.보다 정교한 모델은 강화 학습을 통해 정책을 학습시킵니다. 수행 할 조치를 결정하기 위해 강화 학습 시스템은 대화가 끝날 때 보상을 받고 해당 보상을 사용하여 조치를 취할 정책을 훈련시킵니다. 예를 들어 Fazel-Zarandi et al.의 영화 추천 대화 시스템에서. (2017), 작업 공간에는 EXECUTE, CONFIRM 및 ELICIT의 세 가지 작업 만 있습니다. EXECUTE는 데이터베이스에 쿼리를 전송하고 사용자의 질문에 대답합니다. CONFIRM은 사용자와의 의도 또는 슬롯을 명시하고 (예 : "Christopher Nolan이 감독 한 영화를 원하십니까?") ELICIT가 사용자에게 누락 된 정보 (예 : " 어떤 영화에 대해 이야기하고 있습니까?"). 대화 시스템이 마지막에 올바른 슬롯 표현으로 종료되면 시스템은 큰 긍정적 보상을, 슬롯이 잘못되면 큰 부정적인 보상, 시스템이 모든 것을 재확인하지 못하게하는 확인 및 도출 질문에 대한 작은 부정적인 보상 .

#### Policy Example: Confirmation and Rejection

Modern dialogue systems often make mistakes. It is therefore important for dialogue systems to make sure that they have achieved the correct interpretation of the user’s input. This is generally done by two methods: confirming understandings with the user and rejecting utterances that the system is likely to have misunderstood. As we saw in the prior section, most systems introduce particular strategies and actions related to confirmation and rejection. When using the explicit confirmation explicit confirmation strategy, a system asks the user a direct question to confirm the system’s understanding, like the two examples below in which the system asks a (boldface) yes-no confirmation questions:

현대의 대화 시스템은 종종 실수를합니다. 따라서 대화 시스템이 사용자의 입력을 올바르게 해석했는지 확인하는 것이 중요합니다. 이는 일반적으로 사용자와의 이해를 확인하고 시스템이 오해했을 가능성이있는 발언을 거부하는 두 가지 방법으로 수행됩니다. 이전 섹션에서 보았 듯이 대부분의 시스템은 확인 및 거부와 관련된 특정 전략 및 조치를 소개합니다. 명시 적 확인 명시 적 확인 전략을 사용하는 경우 시스템은 사용자에게 시스템 이해를 확인하기위한 직접적인 질문을합니다 (예 : 시스템에서 예 아니오 확인 질문).

```
S: Which city do you want to leave from?
U: Baltimore.
S: Do you want to leave from Baltimore?
U: Yes.
U: I’d like to fly from Denver Colorado to New York City on September
twenty first in the morning on United Airlines
S: Let’s see then. I have you going from Denver Colorado to New York
on September twenty first. Is that correct?
U: Yes
```

When using the implicit confirmation strategy, a system instead can demon- implicit confirmation strate its understanding as a grounding strategy, for example repeating back the system’s understanding as part of asking the next question, as in the two examples below:

암시 적 확인 전략을 사용하는 경우, 시스템은 대신에 아래의 두 가지 예에서와 같이 다음 질문을하는 과정에서 시스템의 이해를 되풀이하는 것과 같이 암시 적 확인이 접지 전략으로서의 이해를 설명 할 수 있습니다.

```
U: I want to travel to Berlin
S: When do you want to travel to Berlin?
U2: Hi I’d like to fly to Seattle Tuesday Morning
A3: Traveling to Seattle on Tuesday, August eleventh in the morning.
Your full name?
```

Explicit and implicit confirmation have complementary strengths. Explicit confirmation makes it easier for users to correct the system’s misrecognitions since a user can just answer "no" to the confirmation question. But explicit confirmation is awkward and increases the length of the conversation (Danieli and Gerbino 1995, Walker et al. 1998). The explicit confirmation dialogue fragments above sound nonnatural and definitely non-human; implicit confirmation is much more conversationally natural. Confirmation is just one kind of conversational action by which a system can rejection express lack of understanding. Another option is rejection, in which a system gives the user a prompt like I’m sorry, I didn’t understand that. Sometimes utterances are rejected multiple times. This might mean that the user is using language that the system is unable to follow. Thus, when an utterance is rejected, systems often follow a strategy of progressive prompting or escalating progressive prompting detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example from Cohen et al. (2004):

명시 적 및 암시 적 확인에는 보완적인 장점이 있습니다. 명시 적 확인을 통해 사용자는 확인 질문에 "아니오"라고 대답 할 수 있으므로 시스템의 오인을 쉽게 수정할 수 있습니다. 그러나 명백한 확인은 어색하고 대화의 길이를 증가시킵니다 (Danieli and Gerbino 1995, Walker et al. 1998). 위의 명백한 확인 대화 조각은 비 자연스럽고 확실히 비인간적입니다. 암시 적 확인은 훨씬 대화 적으로 자연 스럽습니다. 확인은 시스템이 이해 부족을 거부 할 수있는 일종의 대화 행동입니다. 또 다른 옵션은 거부입니다. 시스템에서 사용자에게 죄송하지만 이해하지 못하는 메시지를 표시합니다. 때로는 발언이 여러 번 거부됩니다. 이는 사용자가 시스템이 따르지 못하는 언어를 사용하고 있음을 의미 할 수 있습니다. 따라서, 발언이 거부 될 때, 시스템은 종종 Cohen et al. (2004) :

```
System: When would you like to leave?
Caller: Well, um, I need to be in New York in time for the first World Series game.
System: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like
to leave.
Caller: I wanna go on October fifteenth.
```

In this example, instead of just repeating "When would you like to leave?", the rejection prompt gives the caller more guidance about how to formulate an utterance the system will understand. These you-can-say help messages are important in helping improve systems’ understanding performance (Bohus and Rudnicky, 2005). If the caller’s utterance gets rejected yet again, the prompt can reflect this ("I still didn’t get that"), and give the caller even more guidance. An alternative strategy for error handling is rapid reprompting, in which the rapid reprompting system rejects an utterance just by saying "I’m sorry?" or "What was that?" Only if the caller’s utterance is rejected a second time does the system start applying progressive prompting. Cohen et al. (2004) summarize experiments showing that users greatly prefer rapid reprompting as a first-level error prompt. It is common to use rich features other than just the dialogue state representation to make policy decisions. For example, the confidence that the ASR system assigns to an utterance can be used by explicitly confirming low-confidence sentences. As we’ll see in page ??, confidence is a metric that the speech recognizer can assign to its transcription of a sentence to indicate how confident it is in that transcription. Confidence is often computed from the acoustic log-likelihood of the utterance (greater probability means higher confidence), but prosodic features can also be used in confidence prediction. For example, utterances with large F0 excursions or longer durations, or those preceded by longer pauses, are likely to be misrecognized (Litman et al., 2000). Another common feature in confirmation is the cost of making an error. For example, explicit confirmation is common before a flight is actually booked or money in an account is moved. Systems might have a four-tiered level of confidence with three thresholds α, β, and γ:

이 예에서는 "종료하고 싶은 시점"을 반복하는 대신 거부 메시지가 발신자에게 시스템이 이해할 수있는 발언을 구성하는 방법에 대한 추가 지침을 제공합니다. 이러한 도움 말 메시지는 시스템의 이해 능력을 향상시키는 데 중요합니다 (Bohus and Rudnicky, 2005). 발신자의 발언이 다시 거부되면 프롬프트에이를 반영하고 ("아직 얻지 못했습니다") 발신자에게 더 많은 안내를 제공 할 수 있습니다. 오류 처리를위한 대체 전략은 빠른 재 프롬 프팅입니다. 빠른 재 프롬 프팅 시스템은 "죄송합니다"라고 말하여 발언을 거부합니다. 또는 "무엇입니까?" 발신자의 발언이 두 번째로 거부 된 경우에만 시스템에서 점진적 프롬프트 적용을 시작합니다. 코헨 (Cohen) 등. (2004)는 사용자가 첫 번째 수준의 오류 프롬프트로 신속한 재 프롬 프팅을 선호한다는 것을 보여주는 실험을 요약합니다. 대화 상태 표현 이외의 다양한 기능을 사용하여 정책을 결정하는 것이 일반적입니다. 예를 들어, ASR 시스템이 발화에 할당한다는 확신은 신뢰도가 낮은 문장을 명시 적으로 확인하여 사용할 수 있습니다. 페이지에서 알 수 있듯이, 신뢰는 음성 인식기가 해당 전사에 얼마나 확신하는지 표시하기 위해 문장의 전사에 할당 할 수있는 지표입니다. 자신감은 종종 발화의 음향 로그 가능성에서 계산되지만 (확률이 높을수록 신뢰도가 높음) 신뢰 예측에 프로 소딕 기능을 사용할 수도 있습니다. 예를 들어, F0 소풍이 길거나 지속 시간이 길거나 일시 정지가 긴 발언은 잘못 인식 될 수 있습니다 (Litman et al., 2000). 확인의 또 다른 일반적인 특징은 오류를 만드는 비용입니다. 예를 들어, 항공편이 실제로 예약되거나 계좌의 돈이 이동되기 전에 명시적인 확인이 일반적입니다. 시스템은 3 가지 임계 값 α, β 및 γ를 갖는 4 계층 신뢰 수준을 가질 수 있습니다.


```
< α low confidence reject
≥ α above the threshold confirm explicitly
≥ β high confidence confirm implictly
≥ γ very high confidence don’t confirm at all
```

### 26.4.5 Natural language generation in the dialogue-state model

Finally, once the policy has decided what speech act to generate, the natural language generation component needs to generate the text of a response to the user. Once a dialogue act has been decided, we need to generate the text of the response to the user. The task of natural language generation (NLG) in the informationstate architecture is often modeled in two stages, content planning (what to say), content planning and sentence realization (how to say it). sentence realization Here we’ll assume content planning has been done by the dialogue policy, which has chosen the dialogue act to generate, and chosen some attributes (slots and values) that the planner wants to say to the user (either to give the user the answer, or as part of a confirmation strategy). Fig. 26.15 shows some sample input/outputs for the sentence realization phase. In the first example, the content planner has chosen the dialogue act RECOMMEND and some particular slots (name, neighborhood, cuisine) and their fillers. The goal of the sentence realizer is to generate a sentence like lines 1 or 2 shown in the figure, by training on many such examples of representation/sentence pairs from a large corpus of labeled dialogues.  Training data is hard to come by; we are unlikely to see every possible restaurants with every possible attribute in many possible differently worded sentences. Therefore it is common in sentence realization to increase the generality of the training.

마지막으로, 정책이 어떤 음성 행위를 생성 할 것인지 결정하면 자연어 생성 컴포넌트는 사용자에 대한 응답 텍스트를 생성해야합니다. 대화 행위가 결정되면 사용자에 대한 응답 텍스트를 생성해야합니다. 정보 상태 아키텍처에서 자연어 생성 (NLG) 작업은 종종 콘텐츠 계획 (무엇을 말해야하는지), 콘텐츠 계획 및 문장 실현 (말을 어떻게하는지)의 두 단계로 모델링됩니다. 문장 실현 여기서 우리는 컨텐츠 계획이 대화 정책에 의해 수행되었다고 가정합니다.이 정책은 생성 할 대화 동작을 선택하고 플래너가 사용자에게 말하고 싶은 일부 속성 (슬롯 및 값)을 선택했습니다 (사용자에게 제공) 답변 또는 확인 전략의 일부로). 그림 26.15는 문장 실현 단계에 대한 일부 샘플 입출력을 보여줍니다. 첫 번째 예에서 컨텐츠 플래너는 대화법 권장 사항과 일부 특정 슬롯 (이름, 이웃, 요리) 및 해당 필러를 선택했습니다. 문장 인식기의 목표는 라벨이 붙은 대화의 큰 모음에서 표현 / 문장 쌍에 대한 많은 예제를 학습함으로써 그림에 표시된 선 1 또는 2와 같은 문장을 생성하는 것입니다. 훈련 데이터는 얻기가 어렵다. 우리는 가능한 모든 다르게 표현 된 문장으로 가능한 모든 속성을 가진 모든 가능한 식당을 볼 수 없을 것입니다. 따라서 훈련의 일반성을 높이는 것이 문장 실현에서 일반적입니다.


```
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 Au Midi is in Midtown and serves French food.
2 There is a French restaurant in Midtown called Au Midi.
recommend(restaurant name= Loch Fyne, neighborhood = city
centre, cuisine = seafood)
3 Loch Fyne is in the City Center and serves seafood food.
4 There is a seafood restaurant in the City Centre called Loch Fyne.
```

Figure 26.15 Two examples of inputs to the sentence realization phase of NLG, showing the dialogue act and attributes prespecified by the content planner. Line 1-2 and 3-4 show different possible output sentences to be generated by the sentence realizer. From the restaurant recommendation system of Nayak et al. (2017).   delexicalization examples by delexicalization. Delexicalization is the process of replacing specific words in the training set that represent slot values with a generic placeholder token representing the slot. Fig. 26.16 shows the result of delexicalizing the training sentences in Fig. 26.15.

그림 26.15 NLG의 문장 구현 단계에 대한 입력의 두 가지 예. 컨텐츠 계획자가 미리 지정한 대화 동작 및 속성을 보여줍니다. 1-2 행과 3-4 행은 문장 인식자가 생성 할 수있는 다른 가능한 출력 문장을 보여줍니다. Nayak 등의 레스토랑 추천 시스템에서. (2017). delexicalization에 의한 delexicalization 예제. Delexicalization은 슬롯 값을 나타내는 트레이닝 세트의 특정 단어를 슬롯을 나타내는 일반 플레이스 홀더 토큰으로 바꾸는 프로세스입니다. 그림 26.16은 그림 26.15의 훈련 문장을 구체화 한 결과를 보여줍니다.

```
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 restaurant name is in neighborhood and serves cuisine food.
2 There is a cuisine restaurant in neighborhood called restaurant name.
```

Figure 26.16 Delexicalized sentences that can be used for generating many different relexicalized sentences. From the restaurant recommendation system of Nayak et al. (2017).

그림 26.16 여러 다른 문장 화 된 문장을 생성하는 데 사용할 수있는 문장 화 된 문장. Nayak 등의 레스토랑 추천 시스템에서. (2017).

Mapping from frames to delexicalized sentences is generally done by encoder decoder models (Wen et al. 2015a, Wen et al. 2015b, Mrksiˇ c et al. 2017 ´ , inter alia), trained on large hand-labeled corpora of task-oriented dialogue (Budzianowski et al., 2018). The input to the encoder is a sequence of tokens xt that represent the dialogue act and its arguments. Thus the attribute/value pairs decor:decent, service:good, cuisine: null might be represented as a flat sequence of tokens, each mapped to a learned embedding wt , as shown in Fig. 26.17.

프레임에서 비선형 문장으로의 매핑은 일반적으로 작업 지향 대화의 큰 수작업 레이블 코포 라에 대해 훈련 된 인코더 디코더 모델 (Wen et al. 2015a, Wen et al. 2015b, Mrksiˇ c et al. 2017 ', 특히)에 의해 수행됩니다. (Budzianowski et al., 2018). 엔코더에 대한 입력은 대화 행위와 그 인수를 나타내는 일련의 토큰 xt입니다. 따라서 속성 / 값 쌍 decor : decent, service : good, 요리 : null은 그림 26.17에 표시된 것처럼 학습 된 임베딩 wt에 각각 매핑 된 일련의 토큰으로 표시 될 수 있습니다.

```
name has decent service
recommend name good
hn
service
```

Figure 26.17 An encoder decoder sentence realizer mapping slots/fillers to English.

The encoder reads all the input slot/value representations, produces a context vector that is used as input to the lexical decoder, which generates an English sentence Let’s suppose in this case we produce the following (delexicalized) sentence:

인코더는 모든 입력 슬롯 / 값 표현을 읽고 어휘 디코더에 대한 입력으로 사용되는 컨텍스트 벡터를 생성합니다.이 문장은 영어 문장을 생성합니다.이 경우 다음과 같은 (비정형 화 된) 문장을 생성한다고 가정하겠습니다

```
restaurant name has decent service
```

Then once we’ve generated the delexicalized string, we can use the input frame relexicalize from the content planner to relexicalize (fill in the exact restaurant or neighborhood or cuisine). This sentence is then relexicalized from the true values in the input frame, resulting in the final sentence:

그런 다음 delexicalized 문자열을 생성 한 후에는 컨텐츠 플래너의 입력 프레임 relexicalize를 사용하여 relexicalize 할 수 있습니다 (정확한 식당이나 이웃 또는 요리를 채우십시오). 그런 다음이 문장은 입력 프레임의 실제 값에서 재구성되어 최종 문장이됩니다.

```
Au Midi has decent service
```

```
TTS Performance Was the system easy to understand ?
ASR Performance Did the system understand what you said?
Task Ease Was it easy to find the message/flight/train you wanted?
Interaction Pace Was the pace of interaction with the system appropriate?
User Expertise Did you know what you could say at each point?
System Response How often was the system sluggish and slow to reply to you?
Expected Behavior Did the system work the way you expected it to?
Future Use Do you think you’d use the system in the future?
```

Figure 26.18 User satisfaction survey, adapted from Walker et al. (2001).

#### Generating Clarification Questions

It’s also possible to design NLG algorithms that are specific to a particular dialogue act. For example, consider the task of generating clarification questions, in cases clarification questions where the speech recognition fails to understand some part of the user’s utterance. While it is possible to use the generic dialogue act REJECT ("Please repeat", or "I don’t understand what you said"), studies of human conversations show that humans instead use targeted clarification questions that reprise elements of the misunderstanding (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013). For example, in the following hypothetical example the system reprises the words "going" and "on the 5th" to make it clear which aspect of the user’s turn the system needs to be clarified:

특정 대화 행위에 고유 한 NLG 알고리즘을 설계 할 수도 있습니다. 예를 들어, 음성 인식이 사용자 발화의 일부를 이해하지 못하는 설명 질문 인 경우 설명 질문을 생성하는 작업을 고려하십시오. 일반적인 대화법 REJECT ("반복하십시오"또는"당신이 말한 것을 이해하지 못합니다")를 사용하는 것이 가능하지만, 인간 대화에 대한 연구는 인간이 오해의 요소를 되 찾는 표적화 된 설명 질문을 사용한다는 것을 보여줍니다 (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013). 예를 들어, 다음과 같은 가상의 예에서 시스템은 사용자의 방향을 명확하게하기 위해 "going"및 "5th on"이라는 단어를 다시 표시합니다.

```
User: What do you have going to UNKNOWN WORD on the 5th?
System: Going where on the 5th?
```

Targeted clarification questions can be created by rules (such as replacing "going to UNKNOWN WORD" with "going where") or by building classifiers to guess which slots might have been misrecognized in the sentence (Chu-Carroll and Carpenter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015).

명확한 설명 질문은 규칙 (예 : "알 수없는 단어로 이동"을 "가는 곳으로"으로 대체하는 등) 또는 문장에서 잘못 인식 된 슬롯을 추측하기위한 분류기를 구축하여 만들 수 있습니다 (Chu-Carroll and Carpenter 1999, Stoyanchev et al. 2014, Stoyanchev 및 Johnston 2015).

## 26.5 Evaluating Dialogue Systems

Evaluation is crucial in dialogue system design. If the task is unambiguous, we can simply measure absolute task success (did the system book the right plane flight, or put the right event on the calendar). To get a more fine-grained idea of user happiness, we can compute a user satisfaction rating, having users interact with a dialogue system to perform a task and then having them complete a questionnaire. For example, Fig. 26.18 shows sample multiple-choice questions (Walker et al., 2001); responses are mapped into the range of 1 to 5, and then averaged over all questions to get a total user satisfaction rating. It is often economically infeasible to run complete user satisfaction studies after every change in a system. For this reason, it is useful to have performance evaluation heuristics that correlate well with human satisfaction. A number of such factors and heuristics have been studied, often grouped into two kinds of criteria: how well the system allows users to accomplish their goals (maximizing task success) with the fewest problems (minimizing costs):

대화 시스템 디자인에서 평가는 매우 중요합니다. 작업이 분명한 경우 절대 작업 성공을 측정 할 수 있습니다 (시스템에서 올바른 비행기 비행을 예약했거나 달력에 올바른 이벤트를 넣었는지). 사용자 만족도에 대한보다 세분화 된 아이디어를 얻기 위해 사용자 만족도를 계산하여 사용자가 대화 시스템과 상호 작용하여 작업을 수행 한 다음 설문지를 작성하도록 할 수 있습니다. 예를 들어, 그림 26.18은 샘플 객관식 문제를 보여줍니다 (Walker et al., 2001). 응답은 1에서 5까지의 범위로 매핑 된 후 모든 질문에 대한 평균을 계산하여 총 사용자 만족도를 얻습니다. 시스템이 변경 될 때마다 완전한 사용자 만족도 조사를 실행하는 것은 종종 경제적으로 불가능합니다. 이러한 이유로, 인간의 만족도와 관련이있는 성능 평가 휴리스틱 스를 갖는 것이 유용합니다. 이러한 여러 가지 요소와 휴리스틱이 연구되었으며 종종 두 가지 종류의 기준으로 그룹화되었습니다. 시스템을 통해 사용자가 최소한의 문제로 목표를 달성하고 (작업 성공을 극대화) 얼마나 비용을 최소화하는지 (비용 최소화) :

#### Task completion success:

Task success can be measured by evaluating the correctness of the total solution. For a frame-based architecture, this might be slot error rate the percentage of slots that were filled with the correct values:

전체 솔루션의 정확성을 평가하여 작업 성공을 측정 할 수 있습니다. 프레임 기반 아키텍처의 경우 올바른 값으로 채워진 슬롯의 백분율 인 슬롯 오류율 일 수 있습니다.

```
Slot Error Rate for a Sentence =
# of inserted/deleted/subsituted slots
# of total reference slots for sentence
```

(26.8)

For example consider a system given this sentence:

예를 들어 다음 문장이 주어진 시스템을 생각해보십시오.

```
(26.9) Make an appointment with Chris at 10:30 in Gates 104
```
which extracted the following candidate slot structure:

```
Slot Filler
PERSON Chris
TIME 11:30 a.m.
ROOM Gates 104
```

Here the slot error rate is 1/3, since the TIME is wrong. Instead of error rate, slot precision, recall, and F-score can also be used. Interestingly, sometimes the user’s perception of whether they completed the task is a better predictor of user satisfaction than the actual task completion success. (Walker et al., 2001). A perhaps more important, although less fine-grained, measure of success is an extrinsic metric like task error rate. In this case, the task error rate would quantify how often the correct meeting was added to the calendar at the end of the interaction.

TIME이 잘못되었으므로 슬롯 오류율은 1/3입니다. 에러율 대신 슬롯 정밀도, 리콜 및 F- 점수를 사용할 수도 있습니다. 흥미롭게도 때로는 작업 완료 여부에 대한 사용자의 인식이 실제 작업 완료 성공보다 사용자 만족도를 더 잘 예측할 수 있습니다. (Walker et al., 2001). 덜 중요하지만 성공의 척도는 작업 오류율과 같은 외래 적 지표입니다. 이 경우 작업 오류율은 상호 작용 종료시 일정 관리에 올바른 미팅이 추가 된 빈도를 수량화합니다.

#### Efficiency cost:

Efficiency costs are measures of the system’s efficiency at helping users. This can be measured by the total elapsed time for the dialogue in seconds, the number of total turns or of system turns, or the total number of queries (Polifroni et al., 1992). Other metrics include the number of system non-responses and the "turn correction ratio": the number of system or user turns that were used solely to correct errors divided by the total number of turns (Danieli and Gerbino 1995, Hirschman and Pao 1993).

효율성 비용은 사용자를 돕는 시스템 효율성의 척도입니다. 이것은 대화의 총 경과 시간 (초), 총 회전 수 또는 시스템 회전 수 또는 총 쿼리 수로 측정 할 수 있습니다 (Polifroni et al., 1992). 다른 메트릭에는 시스템 비 응답 수 및 "턴 수정 비율"이 포함됩니다. 오류를 수정하는 데만 사용 된 시스템 또는 사용자 회전 수를 총 회전 수로 나눈 값 (Danieli and Gerbino 1995, Hirschman and Pao 1993) .

#### Quality cost:

Quality cost measures other aspects of the interactions that affect users’ perception of the system. One such measure is the number of times the ASR system failed to return any sentence, or the number of ASR rejection prompts. Similar metrics include the number of times the user had to barge in (interrupt the system), or the number of time-out prompts played when the user didn’t respond quickly enough. Other quality metrics focus on how well the system understood and responded to the user. The most important is the slot error rate described above, but other components include the inappropriateness (verbose or ambiguous) of the system’s questions, answers, and error messages or the correctness of each question, answer, or error message (Zue et al. 1989, Polifroni et al. 1992).

품질 비용은 사용자의 시스템 인식에 영향을 미치는 상호 작용의 다른 측면을 측정합니다. 그러한 측정 중 하나는 ASR 시스템이 문장을 반환하지 못한 횟수 또는 ASR 거부 프롬프트의 수입니다. 유사한 측정 항목에는 사용자가 끼어 들어야하는 횟수 (시스템 중단) 또는 사용자가 충분히 빨리 응답하지 않았을 때 재생 된 시간 초과 프롬프트 수가 포함됩니다. 다른 품질 지표는 시스템이 사용자를 얼마나 잘 이해하고 응답했는지에 중점을 둡니다. 가장 중요한 것은 위에서 설명한 슬롯 오류율이지만 다른 구성 요소에는 시스템 질문, 답변 및 오류 메시지의 부적합 (자세하거나 모호함) 또는 각 질문, 답변 또는 오류 메시지의 정확성이 포함됩니다 (Zue et al. 1989). , Polifroni et al. 1992).

## 26.6 Dialogue System Design

The user plays a more important role in dialogue systems than in most other areas of speech and language processing, and thus the study of dialogue systems is closely linked with the field of Human-Computer Interaction (HCI). The design of dialogue strategies, prompts, and error messages, is often called voice user interface design, voice user interface and generally follows user-centered design principles (Gould and Lewis, 1985):

사용자는 대부분의 다른 언어 및 언어 처리 영역보다 대화 시스템에서 더 중요한 역할을 수행하므로 대화 시스템에 대한 연구는 HCI (Human-Computer Interaction) 분야와 밀접한 관련이 있습니다. 대화 전략, 프롬프트 및 오류 메시지의 디자인은 종종 음성 사용자 인터페이스 디자인, 음성 사용자 인터페이스라고하며 일반적으로 사용자 중심 디자인 원칙을 따릅니다 (Gould and Lewis, 1985).

1. Study the user and task: Understand the potential users and the nature of the task by interviews with users, investigation of similar systems, and study of related human-human dialogues.

1. 사용자 및 과제 연구 : 사용자와의 인터뷰, 유사한 시스템 조사 및 관련 인간-인간 대화 연구를 통해 잠재적 사용자 및 과제의 성격을 이해합니다.

2. Build simulations and prototypes: A crucial tool in building dialogue systems is the Wizard-of-Oz system. In wizard systems, the users interact with what they Wizard-of-Oz system think is a software agent but is in fact a human "wizard" disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the children’s book The Wizard of Oz (Baum, 1900), in which the wizard turned out to be just a simulation controlled by a man behind a curtain or screen. A Wizard-of-Oz system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, has a graphical interface to a database to run sample queries based on the user utterance, and then has a way to output sentences, either by typing them or by some combination of selecting from a menu and typing. The wizard’s linguistic output can be disguised by a text-to-speech system or, more frequently, by using text-only interactions. The results of a Wizard-of-Oz system can also be used as training data to train a pilot dialogue system. While Wizard-of-Oz systems are very commonly used, they are not a perfect simulation; it is difficult for the wizard to exactly simulate the errors, limitations, or time constraints of a real system; results of wizard studies are thus somewhat idealized, but still can provide a useful first idea of the domain issues.

2. 시뮬레이션 및 프로토 타입 구축 : 대화 시스템을 구축하는 데 중요한 도구는 마법사 시스템입니다. 마법사 시스템에서 사용자는 Wizard-of-Oz 시스템이 소프트웨어 에이전트라고 생각하는 것과 상호 작용하지만 실제로 소프트웨어 인터페이스로 위장한 인간 "마법사"입니다 (Gould et al. 1983, Good et al. 1984, Fraser and 길버트 1991). 그 이름은 어린이 책 The Wizard of Oz (Baum, 1900)에서 나 왔으며, 마법사는 커튼이나 스크린 뒤에있는 사람에 의해 제어되는 시뮬레이션으로 밝혀졌습니다. Oz 마법사는 구현 전에 아키텍처를 테스트하는 데 사용할 수 있습니다. 인터페이스 소프트웨어와 데이터베이스 만 있어야합니다. 마법사는 사용자로부터 입력을 받고, 사용자 발화에 기초하여 샘플 쿼리를 실행하기 위해 데이터베이스에 대한 그래픽 인터페이스를 보유한 다음, 문장을 입력하거나 메뉴에서 선택하여 입력하여 문장을 출력 할 수 있습니다. 마법사의 언어 출력은 텍스트 음성 변환 시스템으로, 또는 더 자주 텍스트 전용 상호 작용을 사용하여 가장 할 수 있습니다. 마법사 마법사 시스템의 결과는 파일럿 대화 시스템을 훈련시키기위한 훈련 데이터로 사용될 수도 있습니다. 오즈의 마법사 시스템은 매우 일반적으로 사용되지만 완벽한 시뮬레이션은 아닙니다. 마법사가 실제 시스템의 오류, 제한 또는 시간 제약을 정확하게 시뮬레이션하는 것은 어렵습니다. 따라서 마법사 연구 결과는 다소 이상적이지만 여전히 도메인 문제에 대한 유용한 첫 번째 아이디어를 제공 할 수 있습니다.

3. Iteratively test the design on users: An iterative design cycle with embedded user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). For example in a famous anecdote in dialogue design history, an early dialogue system required the user to press a key to interrupt the system Stifelman et al. (1993). But user testing showed users barged in, which led to a redesign of the system to recognize overlapped speech. The iterative method is also important for designing prompts that cause the user to respond in normative ways. There are a number of good books on conversational interface design (Cohen et al. 2004, Harris 2005, Pearl 2017).

3. 사용자의 디자인을 반복적으로 테스트 : 시스템 디자인에는 내장 된 사용자 테스트를 통한 반복적 인 디자인주기가 필수적입니다 (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). 예를 들어, 대화 디자인 히스토리의 유명한 일화에서, 초기 대화 시스템은 사용자가 시스템을 중단시키기 위해 키를 누를 것을 요구했다. (1993). 그러나 사용자 테스트 결과 사용자가 끼어 들어 중복 된 음성을 인식하도록 시스템을 다시 디자인했습니다. 반복적 인 방법은 사용자가 규범적인 방식으로 응답하도록하는 프롬프트를 디자인하는 데에도 중요합니다. 대화식 인터페이스 디자인에 관한 많은 훌륭한 책들이 있습니다 (Cohen et al. 2004, Harris 2005, Pearl 2017).

### 26.6.1 Ethical Issues in Dialogue System Design

Ethical issues have long been understood to be crucial in the design of artificial agents, predating the conversational agent itself. Mary Shelley’s classic discussion of the problems of creating agents without a consideration of ethical and humanistic concerns lies at the heart of her novel Frankenstein. One important ethical issue has to do with bias. As we discussed in Section 6.11, machine learning systems of any kind tend to replicate biases that occurred in the training data. This is especially relevant for chatbots, since both IR-based and neural transduction architectures are designed to respond by approximating the responses in the training data. A well-publicized instance of this occurred with MiTay crosoft’s 2016 Tay chatbot, which was taken offline 16 hours after it went live, when it began posting messages with racial slurs, conspiracy theories, and personal attacks. Tay had learned these biases and actions from its training data, including from users who seemed to be purposely teaching it to repeat this kind of language (Neff and Nagy, 2016). Henderson et al. (2017) examined some standard dialogue datasets (drawn from Twitter, Reddit, or movie dialogues) used to train corpus-based chatbots, measuring bias (Hutto et al., 2015) and offensive and hate speech (Davidson et al., 2017). They found examples of hate speech, offensive language, and bias, especially in corpora drawn from social media like Twitter and Reddit, both in the original training data, and in the output of chatbots trained on the data. Another important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. Henderson et al. (2017) point out that home dialogue agents may accidentally record a user revealing private information (e.g. "Computer, turn on the lights –answers the phone –Hi, yes, my password is..."), which may then be used to train a conversational model. They showed that when an encoder-decoder dialogue model is trained on a standard corpus augmented with training keypairs representing private data (e.g. the keyphrase "social security number" followed by a number), an adversary who gave the keyphrase was able to recover the secret information with nearly 100% accuracy. Chatbots that are trained on transcripts of human-human or human-machine conversation must therefore anonymize personally identifiable inIRB formation. It is the role of the Institutional Review Board (IRB) at a researcher’s institution to review research proposals for such ethical issues. Finally, chatbots raise important issues of gender equality. Current chatbots are overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing language, most commercial chatbots evade or give positive responses rather than responding in clear negative ways (Fessler, 2017).

윤리적 인 문제는 오랫동안 인공 에이전트의 디자인에있어 중요한 것으로 이해되어 대화 에이전트 자체를 앞서고있다. 윤리적이고 인본적인 문제를 고려하지 않고 에이전트를 만드는 문제에 대한 Mary Shelley의 고전적인 토론은 소설 Frankenstein의 핵심입니다. 중요한 윤리적 문제 중 하나는 편견과 관련이 있습니다. 6.11 절에서 논의했듯이, 모든 종류의 머신 러닝 시스템은 훈련 데이터에서 발생하는 편견을 모방하는 경향이 있습니다. IR 기반 및 신경 변환 아키텍처는 훈련 데이터의 응답을 근사화하여 응답하도록 설계 되었기 때문에 이는 특히 챗봇과 관련이 있습니다. 잘 알려진이 사례는 MiTay crosoft의 2016 Tay 챗봇에서 생겨난 지 16 시간 만에 오프라인 슬러지, 음모론 및 개인 공격이 포함 된 메시지를 게시하기 시작한 이후에 발생했습니다. Tay는 이러한 종류의 언어를 반복하도록 의도적으로 가르치는 사용자를 포함하여 교육 데이터에서 이러한 편견과 행동을 배웠습니다 (Neff and Nagy, 2016). Henderson et al. (2017)은 말뭉치 기반 챗봇 훈련, 편견 측정 (Hutto et al., 2015) 및 공격적이고 증오심 표현 (Davidson et al., 2017) . 그들은 원래의 훈련 데이터와 데이터에 대해 훈련 된 챗봇의 결과에서 트위터와 레딧과 같은 소셜 미디어에서 얻은 코포 라에서 증오심 표현, 모욕적 인 언어, 편견의 예를 발견했습니다. 또 다른 중요한 윤리적 문제는 프라이버시입니다. Weizenbaum은 이미 ELIZA의 첫날에 사람들의 계시가 챗봇에 미치는 개인 정보 보호의 의미를 지적했습니다. Henderson et al. (2017)은 가정 대화 상담원이 실수로 개인 정보를 공개하는 사용자 (예 : "컴퓨터, 표시 등 켜기 – 전화 받기 – 안녕하세요, 예, 내 비밀번호는 ...")를 기록하여 다음과 같이 사용할 수 있다고 지적합니다. 대화 형 모델을 훈련시킵니다. 그들은 엔코더-디코더 대화 모델이 개인 데이터를 나타내는 훈련 키 쌍 (예 : 키 프레이즈 "사회 보장 번호"와 숫자)으로 보강 된 표준 코퍼스에 대해 훈련 될 때 키 프레이즈를 제공 한 공격자가 비밀을 회복 할 수 있음을 보여 주었다 거의 100 % 정확도로 정보를 제공합니다. 그러므로 인간-인간 또는 인간-기계 대화의 사본에 대해 훈련 된 챗봇은 개인적으로 식별 가능한 inIRB 형성을 익명화해야합니다. 이러한 윤리적 문제에 대한 연구 제안을 검토하는 것은 연구원 기관의 기관 검토위원회 (IRB)의 역할입니다. 마지막으로 챗봇은 성별 평등의 중요한 문제를 제기합니다. 현재 챗봇에는 압도적으로 여성 이름이 주어지며, 하위 여성 종의 고정 관념을 계속 유지할 수 있습니다 (Paolino, 2017). 사용자가 성희롱을 사용하는 경우 대부분의 상업용 챗봇은 명확한 부정적인 방식으로 대응하기보다는 회피하거나 긍정적 인 반응을 나타냅니다 (Fessler, 2017).

## 26.7 Summary

Conversational agents are crucial speech and language processing applications that are already widely used commercially. 

• In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. 
• Chatbots are conversational agents designed to mimic the appearance of informal human conversation. Rule-based chatbots like ELIZA and its modern descendants use rules to map user sentences into system responses. Corpusbased chatbots mine logs of human conversation to learn to automatically map user sentences into system responses. 
• For task-based dialogue, most commercial dialogue systems use the GUS or frame-based architecture, in which the designer specifies frames consisting of slots that the system must fill by asking the user. 
• The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user’s dialogue acts, policies for generating its own dialogue acts, and a natural language component. 
• Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users.

대화 에이전트는 이미 상업적으로 널리 사용되는 중요한 음성 및 언어 처리 응용 프로그램입니다.

• 인간 대화에서 말하는 것은 일종의 행동입니다. 이러한 행위를 언어 행위 또는 대화 행위라고합니다. 화자들은 또한 그들이 서로를 이해했다는 것을 인정함으로써 공통의 기초를 달성하려고 시도합니다. 대화는 또한 턴 구조와 대화 구조를 특징으로합니다.
• 챗봇은 비공식적 인 인간 대화의 모양을 모방하도록 설계된 대화 에이전트입니다. ELIZA와 같은 규칙 기반 챗봇과 최신 자손은 규칙을 사용하여 사용자 문장을 시스템 응답에 매핑합니다. 코퍼스 기반 챗봇은 사용자 대화의 로그를 자동으로 작성하여 사용자 문장을 시스템 응답에 자동으로 매핑하는 방법을 배웁니다.
• 작업 기반 대화의 경우 대부분의 상업용 대화 시스템은 GUS 또는 프레임 기반 아키텍처를 사용합니다. 여기서 디자이너는 사용자에게 시스템에 채워야하는 슬롯으로 구성된 프레임을 지정합니다.
• 대화 상태 아키텍처는 사용자의 대화 동작, 자체 대화 동작 생성 정책 및 자연어 구성 요소를 추적하기위한보다 풍부한 표현과보다 정교한 알고리즘으로 GUS 프레임 및 슬롯 아키텍처를 향상시킵니다.
• 대화 시스템은 일종의 인간-컴퓨터 상호 작용이며, 사용자 역할, Oz 마법사와 같은 시뮬레이션, 실제 사용자에 대한 반복 디자인 및 테스트의 중요성을 포함하여 일반적인 HCI 원칙이 설계에 적용됩니다.